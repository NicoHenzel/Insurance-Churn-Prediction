---
title: "Insurance customer churn prediction"
author: "Nico Henzel"
date: '2022-04-25'
params: 
 institute: "HdM Stuttgart"
 country: "Germany"
 year: "2022"
output:
 html_document: 
  css: style.css # define your own css
  df_print: paged #  tables are printed as HTML tables 
  highlight: default # syntax highlighting style 
  number_sections: yes # numbering of sections
  theme: paper # style option
  fig_height: 4 # figure height
  fig_width: 8 # figure width
  toc: yes # table of content
  toc_float: 
    collapsed: false # show full toc
    smooth_scroll: true # toc scrolling behavior
  # FOOTER erstellen mit Markdown - Tutorial ansehen
  #  includes:
  #   after_body: footer.html # include footer
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(stringr)
library(gt)
library(visdat)
library(skimr)
library(rsample)
library(GGally)
library(tidymodels)
library(xgboost)
library(RCurl)
```

# Plan
Anleitung Siehe https://www.kirenz.com/post/2021-02-17-r-classification-tidymodels/#boosted-tree-xgboost

Offene Punkte Siehe FOlien https://kirenz.github.io/data-science-r/docs/plan.html

Zusätzlich:  

* Bewertungskriterien abdecken aus Slack
* Bewertungskriterien abdecken aus https://drive.google.com/file/d/1AsNHMnbEluH1ikwmp6ulBLqmmiHP3q98/view
## Use case

The goal for this project is to train a classification algorithm with anonymized insurance contract data in order to predict the churn of insurance customers (whether the insurance company will lose a customer or not).
Since the outcome is categorial (customers stays or leaves) and the churn in the available training is labeled, we face a supervised learning problem that can be solved with a classification model.  
In order to measure the performance, the F1-Score will serve as a metric.  

A potential use case, where these insights could be applied, is:
Any insurance company wants to improve (or at least not decrease) their customer loyalty.
If the insurance case handlers could understand why a customer quits a contract and more importantly if a customer is close to quitting an ongoing contract, they could start countermeasures in order to improve customer service and maybe keep the customer.
The output from this model could be provided as information within a dashboard, or implemented into existing applications which display contract information to the case handler. This information can then be used to make further decisions by the case handlers.
The visualization in this project will be realized throug a dashboard.

* To DO: DATA PIPELINE DARSTELLEN
https://docs.google.com/presentation/d/1vjm5YdmOH5LrubFhHf1vlqW2O9Z2UqdWA8biN3e8K5U/edit#slide=id.g19b41f69d7_2_265


## Metrics

Succesful when: 

* Churn prediction on training data > 95 % (not overtrained)
* Visualized recommendations allow case handlers to identify potential churn and get in contact with customers.
* *Can only be tested in a real environment*:  
Churn rate is decreasing after implementation of recommendation.

# Data understanding

## Import

Load training and test dataset (provided from kaggle - see **https://www.kaggle.com/datasets/k123vinod/insurance-churn-prediction-weekend-hackathon**)

```{r train-dataset}
LINK_train <- 
  "https://raw.githubusercontent.com/NicoHenzel/Insurance-Churn-Prediction/main/Data/Train.csv"

training_data <- 
  read_csv(LINK_train)
```

```{r test-dataset}
LINK_test <- 
  "https://raw.githubusercontent.com/NicoHenzel/Insurance-Churn-Prediction/main/Data/Test.csv"

test_data <- 
  read_csv(LINK_test)
```

## Overview
Collect some dimensional (amount of variables and observations) information about the dataframes.

```{r dim-df}
dim_train <- 
  dim(training_data)
dim_test <- 
  dim(test_data)

total_obs <- 
  dim_train[1] + dim_test[1]

train_split <- 
  round(
    100 * (dim_train[1]/total_obs)
    )
test_split <-
  round(
    100 * (dim_test[1]/total_obs)
    )
```

Look at the first rows from each dataframe.

```{r slice-training}
training_data %>% 
  slice_head(n=5) %>% 
  gt()
```

```{r slice-test}
test_data %>% 
  slice_head(n=5) %>% 
  gt()
```

This gives us the following informations:

1. The data seems to be clean already. Further information will be gathered in **Format Data* section.

2. The dimensions for both dataframes are the following:

* Training set:  
  observations = `r dim_train[1]`   
  variables = `r dim_train[2]`
  
* Test set:  
  observations = `r dim_test[1]`  
  variables = `r dim_test[2]`
  
This equals a `r train_split`/`r test_split` split
  
3. The *labels* column is a boolean (1 or 0) which only appears in the training data set.  
This variable can be seen as the indicator for churning, since the column doesn't appear in the given test data (this data should be predicted).


## Clean data

The datasets are already clean with lowercase column names without spaces and no special characters.
This makes it easier for us, since no data cleaning needs to be performed.

To DO
* Jan fragen, ob das in Ordnung ist

## Format data
Next we will be inspecting the given data structure and check if there are any missing values in both datasets.

```{r check-format, warning=FALSE}
glimpse(training_data)
vis_dat(training_data)
glimpse(test_data)
vis_dat(test_data)
na_training <-
  sum(is.na(training_data))
na_test <-
  sum(is.na(test_data))
```

1. The datatype for every variable (feature column) in both datasets is double, although there are variables that only hold integer values (feature_7 to feature_15). This is okay for the training and test datasets, it will be adjusted in the data exploration section in order to get more detailed interpretation of the data.  
2. There are `r na_training` missing values in the training and `r na_test` in the test dataset.  
3. The *labels* column is formatted as numeric (dbl). It should be a factor since it is a categorial variable with two levels (yes or no).  
Let's check this again to be sure:

```{r}
training_data %>% 
  count(labels)
```

First the "label" column will be renamed to "churn" and the binary values are transformed to named values to represent the churn. The column is then transformed into a factor type (which will be needed for running the classification algorithm).

```{r data-modification}
#Rename labels
df_train <-
  training_data %>% 
  rename(churn = labels) %>% 
#Change column type and values
  mutate(
    churn = as.character(churn)
  ) 
# Change values
df_train$churn[df_train$churn == "1"] <- "yes"
df_train$churn[df_train$churn == "0"] <- "no"

# Change column type to factor
df_train <-
  df_train %>% 
  mutate(
    churn = as.factor(churn)
  )

df_train %>% 
  count(churn)

```


## Data split
Since the data comes already with a `r train_split`/`r test_split` split into test and train datasets, we only need to create a validation set before modelling.  
This validation set is used to check the model performance on the training dataset and prevent data leakage.  
The test set will be used in the end in a final fit in order to see the models performance on unused data.

## Data exploration

We create a new dataframe to avoid altering the training dataset during the data exploration. 

```{r}
df_explore <-
  df_train
```

### Data stats

```{r}
skim(df_explore)
```

### Relation to churn variable
TO DO  
* Beziehung der Variablen darstellen und Abhängigkeit zu churn rate

* Siehe https://www.kaggle.com/code/blentalikan/insurancechurnprediction

* Beziehung von bestehenden Variablen darstellen und intepretieren

### Integer Variables

### Float Variables

OFFEN

## List of features


```{r}
# list of all numerical data

list_num <- 
  df_explore %>% 
  select(where(is.numeric)) %>% 
  names()


# list of all categorical data
list_cat <- 
  df_explore %>% 
  select(!where(is.numeric)) %>% 
  names()

length(list_num)
length(list_cat)
```

There are  
`r length(list_cat)` categorial  
`r length(list_num)` numerical  
Variables

To DO
* Integer und Float Unterteilung

# Data preparation

## Data preprocessing recipe

```{r}
# Erstelle ein Rezept für die Modelle, wie mit den Daten umgegangen werden soll
# Modellierung: Zuerst wird die y (Abhängige) Variable geschrieben, dann ~, dann kommen die unabhängigen Variablen (mit einem . können alle anderen Variablen genutzt werden)
# Die Variablen werden folgendermaßen ausgewählt:
# y Variable ist die, die betrachtet werden soll durch das Modell (welche soll ausgewertet werden, abghängig von den anderen Variablen)
# Die restlichen Variablen werden mitgegeben, die aus unserer Sicht die Ergebnis Variable (y) am meisten Beeinflussen

df_rec <-
  recipe(
    churn ~ .,
    data = df_train) %>%
  # Imputation (fehlende Variablen werden durch Median ersetzt) für alle numerischen, außer die Ergebnisvariable (y)
  # Imputationen sollten mit Vorsicht genutzt werden, da viele
   #step_impute_median(all_numeric(), -all_outcomes()) %>%
  # Imputation (fehlende Daten werden mit Mode ersetzt) für alle nominalen Prädikatoren (kategorialen Variablen)
   #step_impute_mode(all_nominal_predictors()) %>%
  # Standard Score z berechnen, um die Variablen untereinander vergleichen zu können
  # Test daten dürfen hier nicht mit normalisiert werden
  # Bei den Test daten dürfen später niemals der Mittelwert berechnet werden, stattdessen nimmt man die Mittelwerte und Standardabweichungen des Trainingsdaten, um die Test Daten zu normalisieren
   #step_normalize(all_numeric(), -all_outcomes()) %>%
  # One Hot Encoding / Bildung von Dummy Variablen - Die Zeilen der kategorialen Variablen werden in Spalten umgewandelt. Diese beinhalten binäre Werte in den Zeilen (ist die Ausprägung vorliegend, oder nicht)
   #step_dummy(all_nominal_predictors()) %>%
  # Korrelationsprüfung, damit die Abhängigkeit der Variablen untereinander geprüft werden
  step_corr(all_predictors(), threshold = 0.7, method = "spearman")

```

```{r}
# Anzeigen, was mit df_rec gemacht werden soll
summary(df_rec)
```

```{r}
prepped_data <- 
  df_rec %>% # use the recipe object
  prep() %>% # perform the recipe on training data
  juice() # extract only the preprocessed dataframe 

glimpse(prepped_data)
```


The recipe is prepared such that the churn variable serves as the outcome which the model should predict.

## Validation set

For the validation set a k-fold crossvalidation with a set of 5 validation folds will be used. Also, the sample is stratified. This makes sure that every characteristic of the data is properly represented in each sample.
The validation set is used in order to check the models performance on the training dataset.  

```{r evaluation-split}
set.seed(42)
cv_folds <-
  vfold_cv(df_train,
           v=5,
           strata = churn,
           breaks = 4)
```


# Model

## Model specification


```{r specification}
xgb_spec <-
  # boost_tree nutzt automatisch die empirisch besten Parameter, wenn keine übergeben werden
  boost_tree() %>%
  set_engine("xgboost") %>%
  # Kann als Klassifikation, oder Regression genutzt werden, deshalb muss das spezifiziert werden
  set_mode("classification")

#show model specification
xgb_spec


```

```{r workflow}
# workflow pipeline
# In die Wf Pipeline wird das recipe aufgenommen (es können auch mehrere sein)
xgb_wflow <-
 workflow() %>%
 add_recipe(df_rec) %>%
 add_model(xgb_spec)
# show workflow
xgb_wflow
```


## Model execution
```{r}

# Erstellt einen Fit der Daten an das Modell und prüft es mit den Folds

xgb_res <-
  xgb_wflow %>%
  fit_resamples(
    # fitte die Daten Anhand der Kreuzvalidierung mit den Folds, die wir vorab festgelegt haben
    resamples = cv_folds,
    # Speichern der Ergebnisse, um die Prediciton anzuschauen
    # Ist nicht immer notwendig
    control = control_resamples(save_pred = TRUE)
  )

```

```{r}
# Zeige die Ergebnisse des Model fits für den Algorithmus xgb an
# rmse gibt Abstand zu Fit des Modells an
# rsq gibt prozentualen Wert an zur Erklärung des Modells für die Datengrundlage
xgb_res %>% collect_metrics(summarize = FALSE)

```

## Model evaluation
TO DO
siehe https://www.kirenz.com/post/2021-02-17-r-classification-tidymodels/#boosted-tree-xgboost
## Predictions

<!-- ```{r} -->
<!-- # Gibt die Prediction für die churn rate bezogen auf die Trainingsdaten -->
<!-- assess_res <- collect_predictions(xgb_res) -->

<!-- assess_res -->

<!-- ``` -->

<!-- ```{r} -->
<!-- # Zeige die top ten der am meisten abweichenden Vorhersagen -->
<!-- # Ermöglicht zusätzliche Daten Evaluierung -->
<!-- wrongest_prediction <- -->
<!--   assess_res %>% -->
<!--   mutate(residual = churn - .) %>% -->
<!--   arrange(desc(abs(residual))) %>% -->
<!--   slice_head(n = 10) -->

<!-- wrongest_prediction -->

<!-- ``` -->


<!-- ```{r} -->
<!-- # Genauere Untersuchung der Abweichungen, indem die Einträge aus der Tabelle der trainings Daten angezeigt werden -->
<!-- # Ermöglicht einzelne Untersuchung der Datenpunkte -->

<!-- errors <- -->
<!--   train_data %>% -->
<!--   dplyr::slice(wrongest_prediction$.row) -->

<!-- ``` -->


# Last evaluation (on test set)

<!-- ```{r} -->
<!-- # Fitte das Modell an die Test Daten, zur Prüfung der Aussagekraft -->
<!-- # Zeigt die Ergebnisse des Models an, indem Daten geprüft werden, die das Model noch nicht kennt -->
<!-- last_fit_xgb <- last_fit(xgb_wflow, split = data_split) -->

<!-- # Show RMSE and RSQ -->
<!-- last_fit_xgb %>% -->
<!--   collect_metrics() -->

<!-- ``` -->

# Additional interpretation
We have seen that our model performs ....  
Since the data provided was anonymized, I used some additional research in order to give an overview of potential features that impact the churn rate.

## Relevant features

<!-- Siehe Google Scholar Veröffentlichungen für relevante Features, da Daten anonym -->
