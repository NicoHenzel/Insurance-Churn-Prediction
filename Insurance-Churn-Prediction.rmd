---
title: "Insurance customer churn prediction"
author: "Nico Henzel"
date: '2022-04-25'
params: 
 institute: "HdM Stuttgart"
 country: "Germany"
 year: "2022"
output:
 html_document: 
  css: style.css # define your own css
  df_print: paged #  tables are printed as HTML tables 
  highlight: default # syntax highlighting style 
  number_sections: yes # numbering of sections
  theme: paper # style option
  fig_height: 4 # figure height
  fig_width: 8 # figure width
  toc: yes # table of content
  toc_float: 
    collapsed: false # show full toc
    smooth_scroll: true # toc scrolling behavior
  # FOOTER erstellen mit Markdown - Tutorial ansehen
  #  includes:
  #   after_body: footer.html # include footer
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(stringr)
library(gt)
library(visdat)
library(skimr)
library(rsample)
library(GGally)
library(tidymodels)
library(xgboost)
library(RCurl)
```

# Plan

<!-- Offene Punkte Siehe FOlien https://kirenz.github.io/data-science-r/docs/plan.html -->
## Use Case

The goal for this project is to train a machine learning algorithm with anonymized insurance contract data in order to predict the insurance customer churn rate (whether the insurance company will lose a customer or not).
A potential use case where these insights could be applied is:
Any insurance company wants to improve (or at least not decrease) their customer loyalty.
If the insurance case handlers could understand why a customer quits a contract and more importantly if a customer is close to quitting an ongoing contract, they could start countermeasures in order to improve customer service and maybe keep the customer.
This information can easily be provided through a dashboard, or implemented into existing applications for viewing contracts, by marking potential customers and contracts which are likely to be quit.
The visualisation in this project will be realized thorug a dashboard.

## Relevant Features

<!-- Siehe Google Scholar Veröffentlichungen für relevante Features, da Daten anonym -->

## Metrics

Succesful when: 

* Churn prediction on training data > 95 % (not overtrained)
* Visualized recommendations allow case handlers to identify potential churn and get in contact with customers.
* *Can only be tested in a real environment*:  
Churn rate is decreasing after implementation of recommendation.

# Data

## Import

Load training and test dataset (provided from kaggle - see *Plan*)

```{r train-dataset}
LINK_train <- 
  "https://raw.githubusercontent.com/NicoHenzel/Insurance-Churn-Prediction/main/Data/Train.csv"

training_data <- 
  read_csv(LINK_train)
```

```{r test-dataset}
LINK_test <- 
  "https://raw.githubusercontent.com/NicoHenzel/Insurance-Churn-Prediction/main/Data/Test.csv"

test_data <- 
  read_csv(LINK_test)
```

## Overview
Look at the first rows from each dataframe.

```{r glimpse-df}
glimpse(training_data)
glimpse(test_data)
```


Get the dimensions for both dataframes.
```{r dim-df}
dim_train <- 
  dim(training_data)
dim_test <- 
  dim(test_data)

total_obs <- 
  dim_train[1] + dim_test[1]
```

This gives us some informations:

1. The datatype for every variable (feature column) in both dataframes is double.   
Although there are variables that only hold integer values (features 7 - 15)

2. The dimensions for both dataframes are the following:

* Training set:  
  observations = `r dim_train[1]`   
  variables = `r dim_train[2]`
  
* Test set:  
  observations = `r dim_test[1]`  
  variables = `r dim_test[2]`
  
* This equals a `r round(100 * (dim_train[1]/total_obs))`/ `r round(100 * (dim_test[1]/total_obs)) ` split
  
3. The *labels* column is a boolean, (1 or 0) which only appears in df_train.  
This can be seen as the indicator for churning, since the column doesn't appear in df_test (where it needs to be tested against).

### Visualize the data
Show any missing values in both dataframes.
```{r}
vis_dat(training_data)
vis_dat(test_data)
```

df_test does not need to be adjusted any further. It can be used as it is to perform our model testing with.

### Clean Data

Does not need to be performed, since data already comes cleaned.

To DO
* Jan fragen, ob das in Ordnung ist

We create a new dataframe to avoid altering the training dataset during the data exploration.  
Afterwards the "label" column is renamed to "churn"

```{r data-modification}
#Create new dataframe 
df_train <- 
  training_data

#Rename labels
df_train <- 
  rename(df_train, churn = labels)

```


## Data split
Since the data comes already split into test and train datasets, we only need to create a validation set to be used during the modeling.  

```{r evaluation-split}

set.seed(42)

cv_folds <-
  vfold_cv(training_data,
           v=5,
           strata = labels,
           breaks = 4)

```


## Data exploration


### Data stats

```{r}
skim(df_train)
```


TO DO  
* Beziehung der Variablen darstellen und Abhängigkeit zu churn rate

* Siehe https://www.kaggle.com/code/blentalikan/insurancechurnprediction

### Create new variable

OFFEN
* Beziehung von bestehenden Variablen darstellen und intepretieren


## Plot data

OFFEN

## List of features


```{r}
# list of all numerical data

list_num <- 
  df_train %>% 
  select(where(is.numeric)) %>% 
  names()


# list of all categorical data
list_cat <- 
  df_train %>% 
  select(!where(is.numeric)) %>% 
  names()

length(list_num)
```

There are  
`r length(list_cat)` categorial  
`r length(list_num)` numerical  
Variables

To DO
* Integer und Float unterteilung


## Data pipeline

```{r}
# Erstelle ein Rezept für die Modelle, wie mit den Daten umgegangen werden soll
# Modellierung: Zuerst wird die y (Abhängige) Variable geschrieben, dann ~, dann kommen die unabhängigen Variablen (mit einem . können alle anderen Variablen genutzt werden)
# Die Variablen werden folgendermaßen ausgewählt:
# y Variable ist die, die betrachtet werden soll durch das Modell (welche soll ausgewertet werden, abghängig von den anderen Variablen)
# Die restlichen Variablen werden mitgegeben, die aus unserer Sicht die Ergebnis Variable (y) am meisten Beeinflussen

df_rec <-
  recipe(
    labels ~ .,
    data = training_data) %>%
  # Imputation (fehlende Variablen werden durch Median ersetzt) für alle numerischen, außer die Ergebnisvariable (y)
  # Imputationen sollten mit Vorsicht genutzt werden, da viele
  # step_impute_median(all_numeric(), -all_outcomes()) %>%
  # Imputation (fehlende Daten werden mit Mode ersetzt) für alle nominalen Prädikatoren (kategorialen Variablen)
  # step_impute_mode(all_nominal_predictors()) %>%
  # Standard Score z berechnen, um die Variablen untereinander vergleichen zu können
  # Test daten dürfen hier nicht mit normalisiert werden
  # Bei den Test daten dürfen später niemals der Mittelwert berechnet werden, stattdessen nimmt man die Mittelwerte und Standardabweichungen des Trainingsdaten, um die Test Daten zu normalisieren
  # step_normalize(all_numeric(), -all_outcomes()) %>%
  # One Hot Encoding / Bildung von Dummy Variablen - Die Zeilen der kategorialen Variablen werden in Spalten umgewandelt. Diese beinhalten binäre Werte in den Zeilen (ist die Ausprägung vorliegend, oder nicht)
  # step_dummy(all_nominal_predictors()) %>%
  # Korrelationsprüfung, damit die Abhängigkeit der Variablen untereinander geprüft werden
  step_corr(all_predictors(), threshold = 0.7, method = "spearman")

```

```{r}
# Anzeigen, was mit df_rec gemacht werden soll
summary(df_rec)
```

The recipe is prepared such that the last variable "labels" serves as the outcome which trains the data.

## Model

```{r}

# model specification

xgb_spec <-
  # boost_tree nutzt automatisch die empirisch besten Parameter, wenn keine übergeben werden
  boost_tree() %>%
  set_engine("xgboost") %>%
  # Kann als Klassifikation, oder Regression genutzt werden, deshalb muss das spezifiziert werden
  set_mode("classification")

# workflow pipeline
# In die Wf Pipeline wird das recipe aufgenommen (es können auch mehrere sein)
xgb_wflow <-
 workflow() %>%
 add_recipe(df_rec) %>%
 add_model(xgb_spec)

```

### Model Execution
HIER WEITER
```{r}

# Erstellt einen Fit der Daten an das Modell und prüft es mit den Folds

xgb_res <-
  xgb_wflow %>%
  fit_resamples(
    # fitte die Daten Anhand der Kreuzvalidierung mit den Folds, die wir vorab festgelegt haben
    resamples = cv_folds,
    # Speichern der Ergebnisse, um die Prediciton anzuschauen
    # Ist nicht immer notwendig
    control = control_resamples(save_pred = TRUE)
  )

```

```{r}
# Zeige die Ergebnisse des Model fits für den Algorithmus xgb an
# rmse gibt Abstand zu Fit des Modells an
# rsq gibt prozentualen Wert an zur Erklärung des Modells für die Datengrundlage
xgb_res %>% collect_metrics(summarize = FALSE)

```

## Predictions

<!-- ```{r} -->
<!-- # Gibt die Prediction für die House Value bezogen auf die Trainingsdaten -->
<!-- assess_res <- collect_predictions(xgb_res) -->

<!-- assess_res -->

<!-- ``` -->

<!-- ```{r} -->
<!-- # Zeige die top ten der am meisten abweichenden Vorhersagen -->
<!-- # Ermöglicht zusätzliche Daten Evaluierung -->
<!-- wrongest_prediction <- -->
<!--   assess_res %>% -->
<!--   mutate(residual = median_house_value - .pred) %>% -->
<!--   arrange(desc(abs(residual))) %>% -->
<!--   slice_head(n = 10) -->

<!-- wrongest_prediction -->

<!-- ``` -->


<!-- ```{r} -->
<!-- # Genauere Untersuchung der Abweichungen, indem die Einträge aus der Tabelle der trainings Daten angezeigt werden -->
<!-- # Ermöglicht einzelne Untersuchung der Datenpunkte -->

<!-- errors <- -->
<!--   train_data %>% -->
<!--   dplyr::slice(wrongest_prediction$.row) -->

<!-- ``` -->


## Final Evaluation

<!-- ```{r} -->
<!-- # Fitte das Modell an die Test Daten, zur Prüfung der Aussagekraft -->
<!-- # Zeigt die Ergebnisse des Models an, indem Daten geprüft werden, die das Model noch nicht kennt -->
<!-- last_fit_xgb <- last_fit(xgb_wflow, split = data_split) -->

<!-- # Show RMSE and RSQ -->
<!-- last_fit_xgb %>% -->
<!--   collect_metrics() -->

<!-- ``` -->
