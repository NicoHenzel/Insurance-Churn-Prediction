---
title: "Insurance customer churn prediction"
author: "Nico Henzel"
date: '2022-04-25'
params: 
 institute: "HdM Stuttgart"
 country: "Germany"
 year: "2022"
output:
 html_document: 
  css: style.css # define your own css
  df_print: paged #  tables are printed as HTML tables 
  highlight: default # syntax highlighting style 
  number_sections: yes # numbering of sections
  theme: paper # style option
  fig_height: 4 # figure height
  fig_width: 8 # figure width
  toc: yes # table of content
  toc_float: 
    collapsed: false # show full toc
    smooth_scroll: true # toc scrolling behavior
  # FOOTER erstellen mit Markdown - Tutorial ansehen
  #  includes:
  #   after_body: footer.html # include footer
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(stringr)
library(gt)
library(visdat)
library(skimr)
library(rsample)
library(GGally)
library(tidymodels)
library(xgboost)
library(RCurl)
```

# Plan

<!-- Offene Punkte Siehe FOlien https://kirenz.github.io/data-science-r/docs/plan.html -->
## Use Case

The goal for this project is to train a classification algorithm with anonymized insurance contract data in order to predict the churn of insurance customers (whether the insurance company will lose a customer or not).
Since the outcome is categorial (customers stays or leaves) and the churn in the available training is labeled, we face a supervised learning problem that can be solved with a classification model.  
In order to measure the performance, the F1-Score will serve as a metric.  

A potential use case, where these insights could be applied, is:
Any insurance company wants to improve (or at least not decrease) their customer loyalty.
If the insurance case handlers could understand why a customer quits a contract and more importantly if a customer is close to quitting an ongoing contract, they could start countermeasures in order to improve customer service and maybe keep the customer.
The output from this model could be provided as information within a dashboard, or implemented into existing applications which display contract information to the case handler. This information can then be used to make further decisions by the case handlers.
The visualization in this project will be realized throug a dashboard.

* To DO: DATA PIPELINE DARSTELLEN
https://docs.google.com/presentation/d/1vjm5YdmOH5LrubFhHf1vlqW2O9Z2UqdWA8biN3e8K5U/edit#slide=id.g19b41f69d7_2_265


## Relevant Features

<!-- Siehe Google Scholar Veröffentlichungen für relevante Features, da Daten anonym -->

## Metrics

Succesful when: 

* Churn prediction on training data > 95 % (not overtrained)
* Visualized recommendations allow case handlers to identify potential churn and get in contact with customers.
* *Can only be tested in a real environment*:  
Churn rate is decreasing after implementation of recommendation.

# Data understanding

## Import

Load training and test dataset (provided from kaggle - see **https://www.kaggle.com/datasets/k123vinod/insurance-churn-prediction-weekend-hackathon**)

```{r train-dataset}
LINK_train <- 
  "https://raw.githubusercontent.com/NicoHenzel/Insurance-Churn-Prediction/main/Data/Train.csv"

training_data <- 
  read_csv(LINK_train)
```

```{r test-dataset}
LINK_test <- 
  "https://raw.githubusercontent.com/NicoHenzel/Insurance-Churn-Prediction/main/Data/Test.csv"

test_data <- 
  read_csv(LINK_test)
```

## Overview
Collect some dimensional (amount of variables and observations) information about the dataframes.

```{r dim-df}
dim_train <- 
  dim(training_data)
dim_test <- 
  dim(test_data)

total_obs <- 
  dim_train[1] + dim_test[1]
```

Look at the first rows from each dataframe.

```{r slice-training}
training_data %>% 
  slice_head(n=5) %>% 
  gt()
```

```{r slice-test}
test_data %>% 
  slice_head(n=5) %>% 
  gt()
```

This gives us the following informations:

1. The datatype for every variable (feature column) in both dataframes is double.   
Since everything is numeric, this is okay, although there are variables that only hold integer values (features 7 - 15).

2. The dimensions for both dataframes are the following:

* Training set:  
  observations = `r dim_train[1]`   
  variables = `r dim_train[2]`
  
* Test set:  
  observations = `r dim_test[1]`  
  variables = `r dim_test[2]`
  
* This equals a `r round(100 * (dim_train[1]/total_obs))`/ `r round(100 * (dim_test[1]/total_obs)) ` split
  
3. The *labels* column is a boolean, (1 or 0) which only appears in the training data set.  
This can be seen as the indicator for churning, since the column doesn't appear in df_test (where it needs to be tested against).
It will later be transformed into a factor, since it will serve as the outcome value in our model.

### Visualize the data
Show any missing values in both dataframes.
```{r}
vis_dat(training_data)
vis_dat(test_data)
```

### Clean Data

The datasets are already clean and without any missing values.
This makes it easier for us, since no data cleaning needs to be performed.

To DO
* Jan fragen, ob das in Ordnung ist

### Format Data

We create a new dataframe to avoid altering the training dataset during the data exploration.  
Afterwards the "label" column is renamed to "churn" and the binary values are transformed to named values to represent the churn. Lastly the column is transformed into a factor type (which will be useful for running the algorithm)

```{r data-modification}
#Create new dataframe 
df_train <- 
  training_data

#Rename labels
df_train <- 
  rename(df_train, churn = labels)

#Change column type and values
df_train <-
  df_train %>% 
  mutate(
    churn = as.character(churn)
  ) 
# Change values
df_train$churn[df_train$churn == "1"] <- "Yes"
df_train$churn[df_train$churn == "0"] <- "No"

# Change column type to factor
df_train <-
  df_train %>% 
  mutate(
    churn = as.factor(churn)
  ) 

```



## Data split
Since the data comes already split into test and train datasets, we only need to create a validation set to be used during the modeling.  

```{r evaluation-split}

set.seed(42)

cv_folds <-
  vfold_cv(df_train,
           v=5,
           strata = churn,
           breaks = 4)

```


## Data exploration


### Data stats

```{r}
skim(df_train)
```


TO DO  
* Beziehung der Variablen darstellen und Abhängigkeit zu churn rate

* Siehe https://www.kaggle.com/code/blentalikan/insurancechurnprediction

### Create new variable

OFFEN
* Beziehung von bestehenden Variablen darstellen und intepretieren


## Plot data

OFFEN

## List of features


```{r}
# list of all numerical data

list_num <- 
  df_train %>% 
  select(where(is.numeric)) %>% 
  names()


# list of all categorical data
list_cat <- 
  df_train %>% 
  select(!where(is.numeric)) %>% 
  names()

length(list_num)
length(list_cat)
```

There are  
`r length(list_cat)` categorial  
`r length(list_num)` numerical  
Variables

To DO
* Integer und Float Unterteilung


## Data pipeline

```{r}
# Erstelle ein Rezept für die Modelle, wie mit den Daten umgegangen werden soll
# Modellierung: Zuerst wird die y (Abhängige) Variable geschrieben, dann ~, dann kommen die unabhängigen Variablen (mit einem . können alle anderen Variablen genutzt werden)
# Die Variablen werden folgendermaßen ausgewählt:
# y Variable ist die, die betrachtet werden soll durch das Modell (welche soll ausgewertet werden, abghängig von den anderen Variablen)
# Die restlichen Variablen werden mitgegeben, die aus unserer Sicht die Ergebnis Variable (y) am meisten Beeinflussen

df_rec <-
  recipe(
    churn ~ .,
    data = df_train) %>%
  # Imputation (fehlende Variablen werden durch Median ersetzt) für alle numerischen, außer die Ergebnisvariable (y)
  # Imputationen sollten mit Vorsicht genutzt werden, da viele
   #step_impute_median(all_numeric(), -all_outcomes()) %>%
  # Imputation (fehlende Daten werden mit Mode ersetzt) für alle nominalen Prädikatoren (kategorialen Variablen)
   #step_impute_mode(all_nominal_predictors()) %>%
  # Standard Score z berechnen, um die Variablen untereinander vergleichen zu können
  # Test daten dürfen hier nicht mit normalisiert werden
  # Bei den Test daten dürfen später niemals der Mittelwert berechnet werden, stattdessen nimmt man die Mittelwerte und Standardabweichungen des Trainingsdaten, um die Test Daten zu normalisieren
   #step_normalize(all_numeric(), -all_outcomes()) %>%
  # One Hot Encoding / Bildung von Dummy Variablen - Die Zeilen der kategorialen Variablen werden in Spalten umgewandelt. Diese beinhalten binäre Werte in den Zeilen (ist die Ausprägung vorliegend, oder nicht)
   #step_dummy(all_nominal_predictors()) %>%
  # Korrelationsprüfung, damit die Abhängigkeit der Variablen untereinander geprüft werden
  step_corr(all_predictors(), threshold = 0.7, method = "spearman")

```

```{r}
# Anzeigen, was mit df_rec gemacht werden soll
summary(df_rec)
```

```{r}
prepped_data <- 
  df_rec %>% # use the recipe object
  prep() %>% # perform the recipe on training data
  juice() # extract only the preprocessed dataframe 

glimpse(prepped_data)
```


The recipe is prepared such that the last variable "labels" serves as the outcome which trains the data.

## Model

```{r}

# model specification

xgb_spec <-
  # boost_tree nutzt automatisch die empirisch besten Parameter, wenn keine übergeben werden
  boost_tree() %>%
  set_engine("xgboost") %>%
  # Kann als Klassifikation, oder Regression genutzt werden, deshalb muss das spezifiziert werden
  set_mode("classification")

# workflow pipeline
# In die Wf Pipeline wird das recipe aufgenommen (es können auch mehrere sein)
xgb_wflow <-
 workflow() %>%
 add_recipe(df_rec) %>%
 add_model(xgb_spec)

```

### Model Execution
HIER WEITER
```{r}

# Erstellt einen Fit der Daten an das Modell und prüft es mit den Folds

xgb_res <-
  xgb_wflow %>%
  fit_resamples(
    # fitte die Daten Anhand der Kreuzvalidierung mit den Folds, die wir vorab festgelegt haben
    resamples = cv_folds,
    # Speichern der Ergebnisse, um die Prediciton anzuschauen
    # Ist nicht immer notwendig
    control = control_resamples(save_pred = TRUE)
  )

```

```{r}
# Zeige die Ergebnisse des Model fits für den Algorithmus xgb an
# rmse gibt Abstand zu Fit des Modells an
# rsq gibt prozentualen Wert an zur Erklärung des Modells für die Datengrundlage
xgb_res %>% collect_metrics(summarize = FALSE)

```

## Predictions

<!-- ```{r} -->
<!-- # Gibt die Prediction für die churn rate bezogen auf die Trainingsdaten -->
<!-- assess_res <- collect_predictions(xgb_res) -->

<!-- assess_res -->

<!-- ``` -->

<!-- ```{r} -->
<!-- # Zeige die top ten der am meisten abweichenden Vorhersagen -->
<!-- # Ermöglicht zusätzliche Daten Evaluierung -->
<!-- wrongest_prediction <- -->
<!--   assess_res %>% -->
<!--   mutate(residual = churn - .) %>% -->
<!--   arrange(desc(abs(residual))) %>% -->
<!--   slice_head(n = 10) -->

<!-- wrongest_prediction -->

<!-- ``` -->


<!-- ```{r} -->
<!-- # Genauere Untersuchung der Abweichungen, indem die Einträge aus der Tabelle der trainings Daten angezeigt werden -->
<!-- # Ermöglicht einzelne Untersuchung der Datenpunkte -->

<!-- errors <- -->
<!--   train_data %>% -->
<!--   dplyr::slice(wrongest_prediction$.row) -->

<!-- ``` -->


## Final Evaluation

<!-- ```{r} -->
<!-- # Fitte das Modell an die Test Daten, zur Prüfung der Aussagekraft -->
<!-- # Zeigt die Ergebnisse des Models an, indem Daten geprüft werden, die das Model noch nicht kennt -->
<!-- last_fit_xgb <- last_fit(xgb_wflow, split = data_split) -->

<!-- # Show RMSE and RSQ -->
<!-- last_fit_xgb %>% -->
<!--   collect_metrics() -->

<!-- ``` -->
