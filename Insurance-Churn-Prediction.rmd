---
title: "Insurance customer churn prediction"
author: "Nico Henzel"
date: '2022-04-25'
params: 
 institute: "HdM Stuttgart"
 country: "Germany"
 year: "2022"
output:
 html_document: 
  css: style.css # define your own css
  df_print: paged #  tables are printed as HTML tables 
  highlight: default # syntax highlighting style 
  number_sections: yes # numbering of sections
  theme: paper # style option
  # fig_height: 4 # figure height
  # fig_width: 8 # figure width
  toc: yes # table of content
  toc_float: 
    collapsed: false # show full toc
    smooth_scroll: true # toc scrolling behavior

---
<!-- Setup -->
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = 1,
  out.width="70%",
  fig.align = "center"
  )
```

<!-- Load packages -->
```{r libraries, include=FALSE}
library(tidyverse)
library(stringr)
library(gt)
library(visdat)
library(skimr)
library(rsample)
library(GGally)
library(tidymodels)
library(xgboost)
library(RCurl)
library(kableExtra)
library(GGally)
library(viridis)
library(reshape2)
library(ggplot2)
library(tibble)
library(purrr)
library(Hmisc)
library(ranger)
library(kknn)
library(performanceEstimation)
library(DT)
library(plotly)
library(vip)
library(htmlwidgets)
```

<!-- Plot theme setting -->
```{r theme-setting, include=FALSE}
theme_set(
  theme_light() + theme()
  )
```

# Plan

## Use case

The goal for this project is to train a classification algorithm with anonymized insurance contract data in order to predict the churn of insurance customers (whether the insurance company will lose a customer or not).
Since the outcome is categorial (customer stays or leaves) and the churn in the available training data is labeled, we face a supervised learning problem that can be solved with a classification model.  

A potential use case, where these insights could be applied, is:  
Any insurance company wants to improve (or at least not decrease) their customer loyalty.
If the insurance case handlers could understand why a customer quits a contract and more importantly if a customer is close to quitting an ongoing contract, they could start countermeasures in order to improve customer service and maybe keep the customer.
The output from this model could be provided as information within a dashboard, or implemented into existing applications which display contract information to the case handler. This information can then be used to make further decisions by the case handlers.
The visualization in this project will be realized through a dashboard made in shiny.

## Metrics

To measure our models performance, we will use the specificity (True negatives / (True negatives + False positives)) and also visualize the predictions with a confusion matrix. The specificity is the most important metric since we want to measure how well our model predicts true negatives (churn) and minimizes false positives (model predicting not churning, when churn would be correct) in order to give the case handlers the best overview of possible churn candidates.
We will train different models and compare their performance based on specificity and the confusion matrix.

The model is successful when: 

* Reasonable portion of churning can be predicted on new data

Additionally we will cover our use case with
* Visualized recommendations that allow case handlers to identify potential churn and get in contact with customers.

# Data understanding

## Import

Load training and test dataset (provided from kaggle - see **https://www.kaggle.com/datasets/k123vinod/insurance-churn-prediction-weekend-hackathon**)

Note: the training dataset will be used to fit the model.
The provided test dataset will be treated as new data to simulate the models performance, since the data is unlabeled. 

```{r load-training-dataset}
LINK_train <- 
  "https://raw.githubusercontent.com/NicoHenzel/Insurance-Churn-Prediction/main/Data/Train.csv"

new_training_data <- 
  read_csv(LINK_train, show_col_types = FALSE)
```

```{r load-new-dataset}
LINK_new <- 
  "https://raw.githubusercontent.com/NicoHenzel/Insurance-Churn-Prediction/main/Data/Test.csv"

new_data <- 
  read_csv(LINK_new, show_col_types = FALSE)
```

## First look
```{r dim-df, echo=FALSE}
dim_train <- 
  dim(new_training_data)
dim_new <- 
  dim(new_data)

total_obs <- 
  dim_train[1] + dim_new[1]

train_split <- 
  round(
    100 * (dim_train[1]/total_obs)
    )
new_split <-
  round(
    100 * (dim_new[1]/total_obs)
    )
```

Look at the first rows from each dataframe.

```{r slice-training}
new_training_data %>% 
  slice_head(n=5) %>% 
  gt() %>% 
  tab_header(
    title = md("**Anonymized contract churn data**"),
    subtitle = md("Training set")
  ) %>% 
  tab_source_note(
    source_note = "Source: Kaggle - Customer Churn Prediction - Weekend Hackathon"
  ) %>% 
    tab_source_note(
    source_note = "https://www.kaggle.com/datasets/k123vinod/insurance-churn-prediction-weekend-hackathon"
    )
  
  
```

```{r slice-test, echo=FALSE}
new_data %>% 
  slice_head(n=5) %>% 
  gt() %>% 
    tab_header(
    title = md("**Anonymized contract churn data**"),
    subtitle = md("Unlabeled dataset")
  ) %>% 
  tab_source_note(
    source_note = "Source: Kaggle - Customer Churn Prediction - Weekend Hackathon"
  ) %>% 
    tab_source_note(
    source_note = "https://www.kaggle.com/datasets/k123vinod/insurance-churn-prediction-weekend-hackathon"
    )
```

This gives us the following informations:

1. The data seems to be clean already. Further information will be gathered in **Format Data** section.

2. The datasets are divided by a `r train_split`/`r new_split` split. The dimensions for both dataframes are the following:

* Training set:  
  observations = `r dim_train[1]`   
  variables = `r dim_train[2]`
  
* New dataset:  
  observations = `r dim_new[1]`  
  variables = `r dim_new[2]`
  
3. The `r text_spec("labels", color = "red")` column is a boolean (1 or 0) which only appears in the training data set.  
This variable can be seen as the indicator for churning, since the column doesn't appear in the given test data.  
This also means, that the provided test data can be trated as new data to simulate a churn prediction on.


## Clean data

The datasets are already clean (lowercase column names without spaces and no special characters).
This makes it easier for us, since no data cleaning needs to be performed.

## Format data
Next we will be inspecting the given data structure and check if there are any missing values in both datasets.

```{r check-format, echo=FALSE, warning=FALSE}
glimpse(new_training_data)
vis_dat(new_training_data)
glimpse(new_data)
vis_dat(new_data)
```

```{r store-na, include=FALSE}
na_training <-
  sum(is.na(new_training_data))
na_new <-
  sum(is.na(new_data))
```


We can see that:

1. The datatype for every variable (feature column) in both datasets is double, although there are variables that only hold integer values (feature_7 to feature_15). In the **Data exploration** section we will inspect the difference between integer and float variables.
2. There are `r na_training` missing values in the training and `r na_new` in the test dataset.  
3. The `r text_spec("labels", color = "red")` column is formatted as numeric (dbl). It should be a factor since it is a categorial variable with two levels (1 or 0).  
First the `r text_spec("labels", color = "red")` column will be renamed to `r text_spec("churn", color = "red")`.  
The column is also transformed into a factor type which is needed for running the classification algorithm.


```{r data-modification}
# Rename labels
df_train <-
  new_training_data %>% 
  rename(churn = labels) %>% 
# Change column type to factor
  mutate(
    churn = as.factor(churn)
  )

```

Note:  
No new variables will be created, since we work with anonymized data.  
The **Data exploration** section covers analysis and interpretation of the relations between the variables.

## Data overview

```{r skim-df-train, echo=FALSE}
skim(df_train) %>% 
  gt()
```

The overview shows:  

* Feature_7 - 15 are discrete and furthermore feature_10 - 12 seem to be binary (1 or 0).
* Every other feature column holds continuous values.
* The maximum and minimum values differ quite a bit, which means the scale for the variables have different scales. This issue will be solved in the **Data preparation** section through feature scaling (data normalization).
* Feature 0-6 have a positive skew shown by the histograms. This does not need to be addressed for our classifier models (normal distributions are not needed).
* The shown stats (mean, sd, perecentiles) are nearly impossible to interpret at this point since we don't know what the variables itself mean.

## Data split

Before exploring the data we perform a data split. The **Data exploration** will only be done on the training set. This is crucial so we don't use any information from the training set in our model building phase. Although the data already comes with a `r train_split`/`r new_split` split the provided test data will not be used for model training since it is unlabeled and better suited to simulate new data.

```{r initial-data-split}

# Seeded split to provide the same split everytime the split is made to make sure we always use the same data for model training.
set.seed(42)
# Split the data with a 75/25 proportion in favor for the training set
data_split <- 
  initial_split(
    df_train, # original training dataset
    prop = 3/4, # 75/25 split
    strata = churn # stratified on churn variable
    )
# Create the dataframes for training and testing
train_data <- training(data_split) 
test_data <- testing(data_split)
```



## Data exploration
In order to get a better understanding of the data and underlying relations between features, we plot the data in different ways.

### Setup
We create a new dataframe to avoid altering the training dataset during the data exploration. 

```{r save-exploration-df}
explore_data <-
  train_data
```

Then we change the binary values to named values to represent the churn in order to make the plots easier to read (Using `r text_spec("yes", color = "green")` and `r text_spec("no", color = "green")` instead of 1 and 0).

```{r change-binary}
# Change column type to character to change values
explore_data <-
  explore_data %>% 
  mutate(
    churn = as.character(churn)
  ) 
# Change values
explore_data$churn[explore_data$churn == "1"] <-
  "yes"
explore_data$churn[explore_data$churn == "0"] <-
  "no"
#Change column type back to factor
explore_data <-
  explore_data %>% 
  mutate(
    churn = as.factor(churn)
  ) 

```


### Churn distribution

The ratio of customers that have churned within the training data set is depicted below.
```{r pie-chart, fig1}
explore_data %>% 
  count(churn, name ="churn_total") %>%
  mutate(percent = churn_total/sum(churn_total)*100,
         percent = round(percent, 2)) %>%
  ggplot(
  aes(x="",
    y=percent,
    fill=churn)
  ) +
  geom_bar(
    stat="identity",
    width=1
    ) +
  coord_polar("y", start = -175) +
  theme_classic() + 
  theme(
    axis.line = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank()
  ) +
  scale_fill_manual(
    values=c("#ffdb58", "#bcd4e6")
      ) + 
  labs(
    x = NULL,
    y = NULL,
    fill = NULL,
    title = "Customer churn distribution",
    subtitle = "For insurance contracts") +
  geom_text(
    aes(label = paste0(percent, "%")),
    position = position_stack(vjust=0.5)
  )

explore_data %>% 
  count(churn, 
        name ="churn_total") %>%
  mutate(percent = churn_total/sum(churn_total)*100,
         percent = round(percent, 2)) %>%
 gt() %>%
  tab_header(
    title = "Insurance customers churn rate ",
    subtitle = "Anonymized training sample"
  ) %>%
  cols_label(
    churn = "Churn",
    churn_total = "Amount",
    percent = "Percent"
  )
```

The data is imbalanced as nearly 90 % are labeled as not churning. We will need to compensate for this before performing hyperparameter tuning.

### Correlation to churn 

The correlation matrix helps us see different relations between the features as it may indicate a predictive relationship between variables (See: https://en.wikipedia.org/wiki/Correlation).
It is formed by calculating the correlation coefficient **r** between each feature. **r** ranges from -1 to 1 and indicates the strength and direction of the linear relation:  
Positive numbers indicate that greater values of one variable lead to greater values of the other variable which also holds true for lower values (similar behavior).  
Negative numbers indicate that greater values of one variable correspond to lesser values of the other (opposite behavior).  
(See: https://en.wikipedia.org/wiki/Correlation and https://en.wikipedia.org/wiki/Covariance)
```{r correlation-matrix, fig2, echo=FALSE}


# Computing correlation coeffeicient matrix - has to be done on new_training_data since correlation is calculated only for numeric values
correlation_matrix <- 
  new_training_data %>%
  # change labels to churn for plot
  rename(churn = labels) %>% 
  # cor() uses the pearson method as default
  cor()

# create correlation matrix with ggplot 
corr_matrx <-
ggplot(
  melt(correlation_matrix),
  aes(Var1, Var2, fill=value)
  ) +
  geom_tile() +
  scale_fill_gradient2(
    low="blue",
    mid="white",
    high="gold"
    ) +
  theme_minimal() +
  coord_equal() +
  labs(
    x= "",
    y= "",
    fill= "Corr",
    title = "Correlation matrix",
    subtitle = "For all features in the training data") +
  theme(
    axis.text.x=element_text(
      size=13,
      angle=90,
      vjust=1,
      hjust=1, 
      margin=margin(-3,0,0,0)
      ),
  axis.text.y=element_text(
    size=13, 
    margin=margin(0,-3,0,0)
    ),
  panel.grid.major=element_blank()
  )

ggplotly(corr_matrx)
```

It is also useful to show the specific correlation of each feature to our churn variable:

```{r churn-correlation-bar-plot, fig3, echo=FALSE}
churn_cor <-
correlation_matrix %>% 
  as.data.frame() %>% 
  select(churn) %>% 
  rename(values = churn) %>% 
  tibble::rownames_to_column() %>% 
  ggplot(
    aes(x = reorder(rowname, -values),
        y = values,
        fill = values)
    )+
  geom_col() + 
  labs(
    x= "Feature",
    y= "Correlation magnitude",
    title = "Correlation to churn",
    subtitle = "In descending order"
    ) +
  # scale_fill_viridis() +
  scale_fill_gradient2(
    low="white",
    mid="blue",
    high="gold"
    ) +
  scale_y_continuous(
    breaks = seq(-0.2, 1, 0.2)
    ) +
  theme(
    axis.text.x=element_text(
      angle=90,
      vjust=0.25,
      hjust=1
      )
    )

ggplotly(churn_cor)
```

We can see that the correlation between `r text_spec("churn", color = "red")` and the other features is the following:

* It correlates not or very weak with most of the features  
* `r text_spec("feature_3", color = "red")` has a moderat positive correlation with `r text_spec("churn", color = "red")`.  
* `r text_spec("feature_5 & 6", color = "red")` correlate slightly positive.  
* `r text_spec("feature_11 & 13", color = "red")` correlate slightly negative.

To further inspect the relation between `r text_spec("churn", color = "red")` and other variables we will create plots for each variable.

### Float Variables
We will start with boxplots and histograms for the float variables.

```{r boxplot-multiplot-float, fig.asp=0.618}
# Credits for the code chunks goes to:
# https://www.kirenz.com/post/2021-02-17-r-classification-tidymodels/#evaluate-models

print_boxplot_float <- function(.y_var){
  
  # convert strings to variable
  y_var <- sym(.y_var) 
 
  # unquote variables using {{}}
  explore_data %>% 
    # Filter out some extreme outliers to better display the plots
    filter(
      feature_1 < 10,
      feature_4 < 8,
      feature_5 < 5,
      feature_6 < 10
      ) %>% 
  ggplot(
    aes(x = churn,
        y = {{y_var}},
        fill = churn,
        color = churn
        )) +
    geom_boxplot(alpha=0.4) +
    labs(
      x = "Churn",
      title = "Relation to churn",
      subtitle = "For every float variable"
    ) +
    theme(legend.position = "none") +
    scale_color_manual(values = c("#ffdb58", "#bcd4e6")) +
    scale_fill_grey()
} 

y_var_float <-
  explore_data %>% 
  # select feature_0 - 6 since those contain float numbers
  select(1:7) %>% 
  # obtain names
  variable.names()

map(y_var_float, print_boxplot_float)
```

```{r bar-multiplot-float, fig.asp=0.618}
# Credits for the code chunks goes to:
# https://www.kirenz.com/post/2021-02-17-r-classification-tidymodels/#evaluate-models


print_bar_float <- function(.x_var){
  
  # convert strings to variable
  x_var <- sym(.x_var) 
 
  # unquote variables using {{}}
  explore_data %>% 
  ggplot(
    aes(x = {{x_var}})) +
    geom_histogram(bins = 20, fill = "blue") +
    labs(
      y = "Frequency",
      title = "Histogram of every float variable",
      subtitle = "To check value range"
    ) 
} 

x_var_float <-
  explore_data %>% 
  # select feature_0 - 6 since those contain float numbers
  select(1:7) %>%
  # obtain names
  variable.names()

map(x_var_float, print_bar_float)
```


This confirms our previous insights from the **Data overview** and **Correlation to churn ** section:  

* Most of the variables do not have a significant influence on the churn label.  

* `r text_spec("feature_3", color = "red")` has a noticeable impact on `r text_spec("churn", color = "red")`. 

* `r text_spec("feature_5 & 6", color = "red")` can be interesting candidates for model training. This will be testes in the **Model** section

* There are alot of outliers present for almost every feature. This can be seen in the histograms as well as the barplots (Note that `r text_spec("features 1, 4, 5 & 6", color = "red")` have been filtered to better display the respective boxplots).

* The scales for float variables are very different.

### Integer Variables

Next up are the boxplots and histograms for the integer variables.

```{r boxplot-function-int, fig.asp=0.618, echo=FALSE}
# Credits for the code chunks goes to:
# https://www.kirenz.com/post/2021-02-17-r-classification-tidymodels/#evaluate-models

print_boxplot_int <- function(.y_var){
  
  # convert strings to variable
  y_var <- sym(.y_var) 
 
  # unquote variables using {{}}
  explore_data %>% 
  ggplot(
    aes(x = churn,
        y = {{y_var}},
        fill = churn,
        color = churn
        )) +
    geom_boxplot(alpha=0.4) +
    labs(
      x = "Churn",
      title = "Relation to churn",
      subtitle = "For every integer variable"
    ) +
    theme(legend.position = "none") +
    scale_color_manual(values = c("#ffdb58", "#bcd4e6")) +
    scale_fill_grey()
} 

y_var_int <-
  explore_data %>% 
  # select feature_0 - 6 since those contain float numbers
  select(8:16) %>% 
  # obtain names
  variable.names()

map(y_var_int, print_boxplot_int)
```


```{r bar-multiplot-int, fig.asp=0.618, echo=FALSE}
# Credits for the code chunks goes to:
# https://www.kirenz.com/post/2021-02-17-r-classification-tidymodels/#evaluate-models


print_bar_int <- function(.x_var){
  
  # convert strings to variable
  x_var <- sym(.x_var) 
 
  # unquote variables using {{}}
  explore_data %>% 
  ggplot(
    aes(x = {{x_var}})) +
    geom_histogram(bins = 20, fill = "blue") +
    labs(
      y = "Frequency",
      title = "Histogram of every int variable",
      subtitle = "To check value range"
    ) 
} 

x_var_int <-
  explore_data %>% 
  # select feature_0 - 6 since those contain float numbers
  select(8:16) %>%
  # obtain names
  variable.names()

map(x_var_int, print_bar_int)
```


Again, this is a confirmation of what we have seen before:

* `r text_spec("feature 11", color = "red")` has a noticeable impact on the `r text_spec("churn", color = "red")` variable.

* `r text_spec("feature 13", color = "red")` can also be an interesting candidate for model training. This will be tested in the **Model** section.

* `r text_spec("feature 7 - 15", color = "red")` contain discrete integer values, in different levels. `r text_spec("feature 10 - 12", color = "red")` contain binary values (0 and 1) for example.

* The scales for integer variables are also different (although not to the same extent as for the float variables).


As a conclusion, we will train our model with 

* `r text_spec("feature 3", color = "red")`
* `r text_spec("feature 5", color = "red")`
* `r text_spec("feature 6", color = "red")`
* `r text_spec("feature 11", color = "red")`
* `r text_spec("feature 13", color = "red")`


# Data preparation

Before model training we will:  

* select our previous identified variables that have an effect on churn  
* create a data split with out freshly gained insights  
* create a recipe to remove outliers and perform feature scaling  
* create a validation set

## Select relevant variables
Select the outcome and predictors for our model
```{r select-relevant-variables}
df_train_new <-
  df_train %>% 
  select(feature_3,
         feature_5,
         feature_6,
         feature_11,
         feature_13,
         churn)
```


## New Data split
The data split will be done once again on the `r text_spec("df_train_new", color = "red")` in order to only contain the relevant features.

```{r data-split}
# Seeded split to provide the same split everytime the split is made to make sure we always use the same data for model training.
set.seed(42)
# Split the data with a 75/25 proportion in favor for the training set
data_split <- 
  initial_split(
    df_train_new, # updated data
    prop = 3/4, # 75/25 split
    strata = churn # stratified on churn variable
    )
# Create the dataframes for training and testing
train_data <- training(data_split) 
test_data <- testing(data_split)
```

## Validation set

For the validation set a k-fold crossvalidation with a set of 10 validation folds will be used. Also, the sample is stratified. This makes sure that every characteristic of the data is properly represented in each sample. The validation set is used in order to check the models performance on the training dataset.

```{r evaluation-split}
# Fix random generation, also see data split
set.seed(42)
# generate the cross validation set
cv_folds <-
  vfold_cv(train_data,
           v=5, # number of folds
           strata = churn, # ensure the same proportion of the churn variable in every fold
           breaks = 4)
```


## Data preprocessing recipe

First we create a recipe for our model that will apply the same steps to all data that we feed into our model.

```{r recipe}
# Create a recipe for our model
churn_rec <-
  recipe(
    # outcome ~ predictor
    churn ~ .,
    data = train_data) %>%
  # perform z-standardization: normalize the numeric variables to have a standard deviation of one and a mean of zero.
  step_normalize(all_numeric(), -all_outcomes()) %>%
  # removes numeric variables that have no variance.
  step_zv(all_numeric(), -all_outcomes()) %>% 
  # remove predictor variables that have large correlations with other predictor variables.
  step_corr(all_predictors(), threshold = 0.7, method = "spearman")

```

These variables with respective roles will be used by our recipe:

```{r show-recipe}
summary(churn_rec)
```

We check how the recipe will affect our training data

```{r check-preprocessing}
prepped_data <-
  churn_rec %>% # use the recipe object
  prep() %>% # apply recipe
  juice() # show the processed data


glimpse(prepped_data)
```

<!-- ```{r prepped-data} -->
<!-- prepped_data %>%  -->
<!--   select(-churn) %>%  -->
<!--   ggscatmat(corMethod = "spearman", -->
<!--             alpha=0.2) -->
<!-- ``` -->

The recipe is prepared such that the churn variable serves as the outcome which the model should predict.
We can see that `r text_spec("feature_5", color = "red")` has been removed. This happened by the step_corr() function in the recipe meaning feature_5 correlates with another feature and is not providing additional useful information in model training.


# Model - XGBoost

## Model specification

We will set up the models with the needed specification.

```{r specification}
xgb_spec <-
  # uses empirically best parameters, if none are specified
  boost_tree() %>%
  # model engine
  set_engine("xgboost") %>%
  # model mode
  set_mode("classification")

# show model specification
xgb_spec
```

## Workflow

Next we create a workflow object to combine the recipe `r text_spec("churn_rec", color = "red") with our model in **Model execution**.

```{r workflow}
# workflow pipeline
xgb_wflow <-
  workflow() %>%
  # specifiy our recipe
  add_recipe(churn_rec) %>%
  # and our model
  add_model(xgb_spec)

# show workflow
xgb_wflow
```

## Model execution

Finally we can execute our model and fit it to our training data. We save the predictions made so we can evaluate the performance metrics, especially the specificity.

```{r execute-model}

# Create fit to the model and check with validation folds

xgb_res <-
  xgb_wflow %>%
  fit_resamples(
    # Estimate performance for specified metrics on all folds by fitting model to each resample while holding out a portion from each resample to evaluate.
    resamples = cv_folds,
    # Define metrics
    metrics = metric_set(
             spec,
             accuracy,
             roc_auc,
             recall,
             precision
      ),
    # Save predictions 
    control = control_resamples(save_pred = TRUE)
  )

```
## Model evaluation

We can evaluate our metrics with `r text_spec("collect_metrics()", color = "red")`.  

The performance over all folds is:

```{r show-metrics, echo=FALSE}
xgb_res %>% 
  collect_metrics(summarize = TRUE)

```

Our models performance overall seems to be good. The only problem is that the specificity is low.

We then collect our predictions. This will be needed to create the confusion matrix and ROC curve

```{r collect-predictions}
xgb_pred <- 
  xgb_res %>% 
  collect_predictions()
```

Next we create the confusion matrix and plot it.  

```{r confusion-matrix}
xgb_pred %>% 
  conf_mat(churn, .pred_class) %>% 
  autoplot(type = "heatmap") +
  labs(
    title = "Confusion matrix",
    subtitle = "XGB model",
    x = "Truth",
    y = "Prediction"
  )

```

Overall our model has a high rate of true positives but (as we have seen already) a low specificity - rate of true negatives.  

We also visualize the ROC:

```{r roc-curve}
xgb_pred %>% 
  group_by(id) %>% # id contains our folds
  roc_curve(churn, .pred_0) %>% 
  autoplot() +
  labs(
    title = "ROC curve",
    subtitle = "XGB model",
    x = "False positive rate (1- specificity)",
    y = "True positive rate (sensitivity)",
    color = "Folds")
```

The ROC curve looks really good, but we need to remember that the rate of predicting true negatives (specificity) is still low.

In order to find a better model we will train and evaluate some other classifying algorithms and compare all models.

# Additional model training

We will use the following algorithms

* Logistic regression
* Random forest
* K-nearest neighbor

We repeat all our steps from above to create specifications, workflows and train the models.

```{r specifications}
# Logistic regression
log_spec <- 
  # will use best empirical hyperparamters
  logistic_reg() %>% 
  set_engine(engine = "glm") %>%
  set_mode("classification") 

# Random forest
rf_spec <- 
  # will use best empirical hyperparamters
  rand_forest() %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

# K-nearest neighbor
knn_spec <- 
  # will use best empirical hyperparamters
  nearest_neighbor(neighbors = 4) %>% # can be adjusted
  set_engine("kknn") %>% 
  set_mode("classification") 

```

```{r workflows}
# Logistic regression
log_wflow <- 
 workflow() %>% 
 add_recipe(churn_rec) %>%  
 add_model(log_spec)

# Random forest
rf_wflow <-
 workflow() %>%
 add_recipe(churn_rec) %>% 
 add_model(rf_spec) 

# K-nearest neighbor
knn_wflow <-
 workflow() %>%
 add_recipe(churn_rec) %>% 
 add_model(knn_spec)

```

```{r fit-models}
# Logistic regression
log_res <-
  log_wflow %>%
  fit_resamples(
    resamples = cv_folds,
    # Define metrics
    metrics = metric_set(
             spec,
             accuracy,
             roc_auc,
             recall,
             precision
      ),
    # Save predictions 
    control = control_resamples(save_pred = TRUE)
  )


# Random forest
rf_res <-
  rf_wflow %>%
  fit_resamples(
    resamples = cv_folds,
    # Define metrics
    metrics = metric_set(
      spec,
      accuracy,
      roc_auc,
      recall,
      precision
      ),
    # Save predictions 
    control = control_resamples(save_pred = TRUE)
  )

# K-nearest neighbor
knn_res <-
  knn_wflow %>%
  fit_resamples(
    resamples = cv_folds,
    # Define metrics
    metrics = metric_set(
             spec,
             accuracy,
             roc_auc,
             recall,
             precision
      ),
    # Save predictions 
    control = control_resamples(save_pred = TRUE)
  )

```


## Model comparison

We compare the models by first looking at the different metrics and the confusion matrices.


* Logistic regression

```{r log-metrics, echo=FALSE}
log_res %>%  collect_metrics(summarize = TRUE)

log_pred <-
  log_res %>% 
  collect_predictions()

log_pred %>% 
  conf_mat(churn, .pred_class) %>% 
  autoplot("heatmap")+
  labs(
    title = "Confusion matrix",
    subtitle = "LOG model",
    x = "Truth",
    y = "Prediction"
  )
# 
# log_pred %>% 
#   group_by(id) %>% # id contains our folds
#   roc_curve(churn, .pred_0) %>% 
#   autoplot() +
#   labs(
#     title = "ROC curve",
#     subtitle = "LOG model",
#     x = "False positive rate (1- specificity)",
#     y = "True positive rate (sensitivity)",
#     color = "Folds")

```

* Random forest

```{r rf-metrics, echo=FALSE}

rf_res %>%  collect_metrics(summarize = TRUE)

rf_pred <-
  rf_res %>% 
  collect_predictions()

rf_pred %>% 
  conf_mat(churn, .pred_class) %>% 
  autoplot(type = "heatmap")+
  labs(
    title = "Confusion matrix",
    subtitle = "RF model",
    x = "Truth",
    y = "Prediction"
  )

# rf_pred %>% 
#   group_by(id) %>% # id contains our folds
#   roc_curve(churn, .pred_0) %>% 
#   autoplot() +
#   labs(
#     title = "ROC curve",
#     subtitle = "RF model",
#     x = "False positive rate (1- specificity)",
#     y = "True positive rate (sensitivity)",
#     color = "Folds")


```

* K-nearest neighbor

```{r knn-metrics, echo=FALSE}
knn_res %>%  collect_metrics(summarize = TRUE)

knn_pred <-
  knn_res %>% 
  collect_predictions()

knn_pred %>% 
  conf_mat(churn, .pred_class) %>% 
  autoplot(type = "heatmap")+
  labs(
    title = "Confusion matrix",
    subtitle = "KNN model",
    x = "Truth",
    y = "Prediction"
  )

# knn_pred %>% 
#   group_by(id) %>% # id contains our folds
#   roc_curve(churn, .pred_0) %>% 
#   autoplot() +
#   labs(
#     title = "ROC curve",
#     subtitle = "KNN model",
#     x = "False positive rate (1- specificity)",
#     y = "True positive rate (sensitivity)",
#     color = "Folds")
```

We can see that our xgboost classification algorithm performed the best out of all models so far, considering specificity and the confusion matrix.

We will use that model and fine tune it in order to gain a better specificity and thus a higher prediction for churning.

# Model fine tuning

To adjust our imbalanced data, we will use the synthetic minority oversampling technique `r text_spec("smote()", color = "red")`.

```{r smote}

smote_data <-
  # Generate more samples of the minority label (churn) while also decreasing samples of the majority label (no churn) 
  smote(churn ~., train_data, perc.over = 3, perc.under = 2)

table(smote_data$churn)
```
The data ist still imbalanced, but not as much as before.

We will also need to create a new evaluation set with the adjusted dataset.
```{r new-evaluation-split}
# Fix random generation, also see data split
set.seed(42)
# generate the cross validation set
cv_folds <-
  vfold_cv(smote_data,
           v=5, # number of folds
           strata = churn, # ensure the same proportion of the churn variable in every fold
           breaks = 4)
```

Now all we have to do is perform hyperparameter tuning our xgboost algorithm to select the best values to maximize specificity.

## Model specification

We have to specify the hyperparameters that we want to tune.
We also adjust for the rest of the imbalance by using `r text_spec("scale_pos_weight", color = "red")`.
```{r tuned-specification}

xgb_tuned_spec <-
  boost_tree(
      mtry = tune(), # number (or proportion) of predictors
      trees = 10, # number of decision trees. For testing, we can leave it at a lower value
      min_n = tune(), # minimum number of data points in a node required to split
      tree_depth = tune(), # maximum splits (depth of decision trees)
      learn_rate = tune(), # adaption rate over iterations
      loss_reduction = tune(), # reduction in loss required to split
      sample_size = tune(), # number (or proportion) of data exposed to fitting
  ) %>%
  # model engine
  # specifiy scale_pos_weight to account for the imbalanced dataset
  set_engine("xgboost", scale_pos_weight = tune()) %>%
  # model mode
  set_mode("classification")

# show model specification
xgb_tuned_spec

```

## Workflow

Next we create a new workflow object.

```{r tuned-workflow, echo=FALSE}
# workflow pipeline
xgb_tuned_wflow <-
  workflow() %>%
  # specifiy our recipe
  add_recipe(churn_rec) %>%
  # and our model
  add_model(xgb_tuned_spec)

# show workflow
xgb_tuned_wflow
```


## Grid search

In order to tune the parameters we first set up possible values for them. For efficiency reasons we use grid_latin_hypercube().  
For more information see:  
https://htmlpreview.github.io/?https://github.com/kirenz/tidymodels-in-r/blob/main/05-tidymodels-xgboost-tuning.html

```{r grid-search}

xgb_grid <- grid_latin_hypercube(
  scale_pos_weight(),
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(), # needs to be a proportion
  finalize(mtry(), smote_data), # different approach since mtry depends on number of predictors in data
  learn_rate(),
  size = 10
)
```


## Hyperparameter tuning

The `r text_spec("tune_grid", color = "red")` function computes the performance metrics for us, which we defined earlier.

```{r tuning}
# Seeded to make sure we can reproduce the tuning
set.seed(42) 

xgb_tuned_res <- tune_grid(
  xgb_tuned_wflow,
  resamples = cv_folds,
  grid = xgb_grid,
  # save the predictions to visualize the data
  metrics = metric_set(
             spec,
             accuracy,
             roc_auc,
             recall,
             precision
      ),
  control = control_grid(save_pred = TRUE) 
)

metrics <-
xgb_tuned_res %>% 
  collect_metrics(summarize = TRUE)
```

## Explore results

We visualize the specificty for every hyperparameter which gives us a first impression of how well specific values perform. 

```{r roc-auc}

# Credits for the code chunk goes to:
# https://htmlpreview.github.io/?https://github.com/kirenz/tidymodels-in-r/blob/main/05-tidymodels-xgboost-tuning.html

# Pivot xgb_tuned_res to longer data format while only filtering for spec
xgb_res_spec <-
  xgb_tuned_res %>%
  collect_metrics() %>%
  filter(.metric == "spec") %>%
  select(mean, mtry:sample_size) %>% 
  pivot_longer(mtry:sample_size,
               values_to = "value",
               names_to = "parameter"
  )
spec_tuning_comp <-
# show specificity side by side for every hyperparameter
xgb_res_spec %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  scale_color_viridis_d()+
  labs(x = NULL,
       y = "Specificity",
       title = "Different values for specificity",
       subtitle = "For each hyperparameter after tuning")

ggplotly(spec_tuning_comp)
```

## Finding the best parameters 

```{r show-spec}
show_best(xgb_tuned_res, 
          "spec")
```

```{r select-best-spec}
best_spec <- 
  select_best(
    xgb_tuned_res, 
    "spec" 
    )
best_spec
```

This set of values will be chosen for our hyperparameters as it overall provides the best specificity values.


## Adding best parameters to Workflow

We create a final workflow which uses our best set of hyperparameters.

```{r add-to-wf}

final_xgb_Wf <- finalize_workflow(
  xgb_tuned_wflow,
  best_spec
)

final_xgb_Wf

```


## Tuned Model execution

Fit the model with our finalized workflow.
```{r execute-tuned-model}

xgb_tuned_res <-
  final_xgb_Wf %>%
  fit_resamples(
    resamples = cv_folds,
    metrics = metric_set(
             spec,
             accuracy,
             roc_auc,
             recall,
             precision
      ),
    # Save predictions 
    control = control_resamples(save_pred = TRUE)
  )

```

## Tuned model evaluation

Evaluate the metrics over all folds and show the confusion matrix and ROC curve.

```{r tuned-model-evaluation, echo=FALSE}
xgb_tuned_res %>% 
  collect_metrics(summarize = TRUE)

xgb_tuned_pred <- 
  xgb_tuned_res %>% 
  collect_predictions()

xgb_tuned_pred %>% 
  conf_mat(churn, .pred_class) %>% 
  autoplot(type = "heatmap") +
  labs(
    title = "Confusion matrix",
    subtitle = "XGB tuned model on training data",
    x = "Truth",
    y = "Prediction"
  )

xgb_tuned_pred %>% 
  group_by(id) %>% # id contains our folds
  roc_curve(churn, .pred_0) %>% 
  autoplot()  +
  labs(
    title = "ROC curve",
    subtitle = "XGB tuned model on training data",
    x = "False positive rate (1- specificity)",
    y = "True positive rate (sensitivity)",
    color = "Folds")
```

We can see that our tuned model outperformed every other model regarding the specificity. Ideally we would achieve around 85-90%, but we already tuned the model to obtain the best hyperparameters.


# Last evaluation

We can now perform our last fit to the test data we split of earlier.

```{r last-fit}
# perform last fit of our model on test data
last_fit_xgb <- 
  last_fit(final_xgb_Wf,
           split = data_split,
           metrics = metric_set(
             spec,
             accuracy,
             roc_auc,
             recall,
             precision
             )
           )
```

To see how our model performed we check the performance metrics

```{r performance-last-fit, echo=FALSE}

last_fit_xgb %>% 
  collect_metrics()

```

```{r confusion-matrix-last-fit, echo=FALSE}
last_fit_xgb %>%
  collect_predictions() %>% 
  conf_mat(churn, .pred_class) %>% 
  autoplot(type = "heatmap")  +
  labs(
    title = "Confusion matrix",
    subtitle = "XGB tuned model on test data",
    x = "Truth",
    y = "Prediction"
  )
# 
# last_fit_xgb %>% 
#   group_by(id) %>% # id contains our folds
#   roc_curve(churn, .pred_0) %>% 
#   autoplot()  +
#   labs(
#     title = "ROC curve",
#     subtitle = "XGB tuned model on test data",
#     x = "False positive rate (1- specificity)",
#     y = "True positive rate (sensitivity)",
#     color = "Folds")
```

The specificity is at a low value which means our model did not predict the churn label well regarding the test data.

```{r vip-features, warning=FALSE}

vip_features <-
last_fit_xgb %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip(num_features = 10)

ggplotly(vip_features)

```

We can see that the most important feature for the prediction was  `r text_spec("feature_3", color = "red")`.
The others have a relatively low importance which is an indicator that feature engineering would have given better results on the test data.

The results point towards an underfitting scenario where our model is not complex enough to accurately predict the churn with the given data. In order to improve the performance further, custom performance metrics or new features could be generated.
Another step would be to use different models (like neural networks) or an ensemble of models. 

Even if the model predicts only about 30% of the actual churning customers, We can still use it for our use case since the cost for not guessing correctly is low. Also any contract labelled as **churning** is better than no predictive information.

# Predictions on new, unlabeled data

We will use our model on new data, which has not been labeled yet. We fit it to the second dataset provided by kaggle which we stored in our `r text_spec("new_data", color = "red")` dataframe.

In order to make predictions, we need to create an object of class model_fit of our `r text_spec("final_xgb_wf", color = "red")`. This is done using the `r text_spec("fit", color = "red")` function.

```{r predictions, message=FALSE, warning=FALSE}
# create final model
final_model <-
  fit(final_xgb_Wf, df_train_new)

# create predictions
predictions <-
predict(final_model, new_data)

# create new dataframe
new_prediction_data <-
  new_data

# add predictions to new_data_prediction
new_prediction_data$churn_prediction <-
  predictions$.pred_class

# put predictions as first column
new_prediction_data <-
  new_prediction_data %>% 
  select(churn_prediction, everything())

```

We change the prediction labels to better represent the churn.

```{r transfrom-predictions}

new_prediction_data <-
  new_prediction_data %>% 
  mutate(
    churn_prediction = as.character(churn_prediction)
  )


new_prediction_data$churn_prediction[new_prediction_data$churn_prediction == 0] <- "no"
new_prediction_data$churn_prediction[new_prediction_data$churn_prediction == 1] <- "yes"


```

Let's take a look at the transformed data

```{r new-data-prediction-labels, warning=FALSE}

datatable(new_prediction_data,
              extensions = list(
                'Buttons' = NULL,
                "Responsive" = NULL
              ),
              options = list(
                dom = 'Bfrtip',
                buttons = c( 'csv', 'excel', 'pdf')
              )
            )
```



Next we recreate the pie chart for our predictions
```{r pie-chart-prediction, echo=FALSE}
pie_chart <-
new_prediction_data %>% 
  count(churn_prediction, name ="churn_total") %>%
  mutate(percent = churn_total/sum(churn_total)*100,
         percent = round(percent, 2)) %>%
  ggplot(
  aes(x="",
    y=percent,
    fill=churn_prediction)
  ) +
  geom_bar(
    stat="identity",
    width=1
    ) +
  coord_polar("y") +
  theme_classic() + 
  theme(
    axis.line = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank()
  ) +
  scale_fill_manual(
    values=c("#ffdb58", "#bcd4e6")
      ) + 
  labs(
    x = NULL,
    y = NULL,
    fill = NULL,
    title = "Predicted customer churn distribution",
    subtitle = "On new dataset") +
  geom_text(
    aes(label = paste0(percent, "%")),
    position = position_stack(vjust=0.5)
  )

pie_chart

new_prediction_data %>% 
  count(churn_prediction, name ="churn_total") %>%
  mutate(percent = churn_total/sum(churn_total)*100,
         percent = round(percent, 2))

```



We then save the predictions on the new data, our pie chart and the plotly graph of our vip features to our Dashboard folder in order to use it in our dashboard.
```{r save-files}
# save data
write.csv(new_prediction_data, "Dashboard\\Results.csv", row.names=FALSE)

# save plotly chart vip-features as html
fig <-
ggplotly(vip_features)

htmlwidgets::saveWidget(fig, 
                        file="Dashboard\\insurance-churn-dashboard\\www\\plotly.html",
                        selfcontained = TRUE)

# save pie chart as png
png("Dashboard\\insurance-churn-dashboard\\www\\pie-chart.png")
par(mar = c(4.1, 4.4, 4.1, 1.9), xaxs="i", yaxs="i")
pie_chart
dev.off()

```



# Conclusion
This report shows how to build a classifier algorithm for anonymized data. We have seen that our model still has room for improvement due to underfitting. The model does not predict churning at a high rate. Depending on the use case this performance has to be enhanced. I suggested, that additional feature engineering and model stacking could lead to better results.

The created dashboard is a mockup in order to display the models predictions to case handlers of contract data.
You can access the source code for this report and the dashboard here:
https://github.com/NicoHenzel/Insurance-Churn-Prediction