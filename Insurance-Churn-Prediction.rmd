---
title: "Insurance customer churn prediction"
author: "Nico Henzel"
date: '2022-04-25'
params: 
 institute: "HdM Stuttgart"
 country: "Germany"
 year: "2022"
output:
 html_document: 
  css: style.css # define your own css
  df_print: paged #  tables are printed as HTML tables 
  highlight: default # syntax highlighting style 
  number_sections: yes # numbering of sections
  theme: paper # style option
  # fig_height: 4 # figure height
  # fig_width: 8 # figure width
  toc: yes # table of content
  toc_float: 
    collapsed: false # show full toc
    smooth_scroll: true # toc scrolling behavior
  # FOOTER erstellen mit Markdown - Tutorial ansehen
  #  includes:
  #   after_body: footer.html # include footer
---
<!-- Setup -->
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = 1,
  out.width="70%",
  fig.align = "center"
  )
```

<!-- Load packages -->
```{r libraries, include=FALSE}
library(tidyverse)
library(stringr)
library(gt)
library(visdat)
library(skimr)
library(rsample)
library(GGally)
library(tidymodels)
library(xgboost)
library(RCurl)
library(kableExtra)
library(GGally)
library(viridis)
library(reshape2)
library(ggplot2)
library(tibble)
library(purrr)
library(Hmisc)
```

<!-- Theme setting -->
```{r theme-setting, include=FALSE}
theme_set(
  theme_light() + theme()
  )
```

# Plan
Anleitung Siehe https://www.kirenz.com/post/2021-02-17-r-classification-tidymodels/#boosted-tree-xgboost

Offene Punkte Siehe FOlien https://kirenz.github.io/data-science-r/docs/plan.html

Zusätzlich:  

* Bewertungskriterien abdecken aus Slack
* Bewertungskriterien abdecken aus https://drive.google.com/file/d/1AsNHMnbEluH1ikwmp6ulBLqmmiHP3q98/view
* Am Schluss: Styling überarbeiten und Abbildungsverzeichnis erstellen
* Dashboard erstellen mit Implementierung der neuen Daten. Inhalte:  
Customer churn distribution (pie Chart)
INTERAKTIVE ABBILDUNG MUSS DEFINIERT WERDEN!
z.B. Korrelation der einzelnen Features mit churn rate darstellen, nach Features und relationshöhe filterbar.
Tabelle mit Zeilen, die als Churn predicted wurden


## Use case

The goal for this project is to train a classification algorithm with anonymized insurance contract data in order to predict the churn of insurance customers (whether the insurance company will lose a customer or not).
Since the outcome is categorial (customers stays or leaves) and the churn in the available training is labeled, we face a supervised learning problem that can be solved with a classification model.  
In order to measure the performance, the F1-Score will serve as a metric.  

A potential use case, where these insights could be applied, is:
Any insurance company wants to improve (or at least not decrease) their customer loyalty.
If the insurance case handlers could understand why a customer quits a contract and more importantly if a customer is close to quitting an ongoing contract, they could start countermeasures in order to improve customer service and maybe keep the customer.
The output from this model could be provided as information within a dashboard, or implemented into existing applications which display contract information to the case handler. This information can then be used to make further decisions by the case handlers.
The visualization in this project will be realized throug a dashboard.

* To DO: DATA PIPELINE DARSTELLEN
https://docs.google.com/presentation/d/1vjm5YdmOH5LrubFhHf1vlqW2O9Z2UqdWA8biN3e8K5U/edit#slide=id.g19b41f69d7_2_265


## Metrics

The performance of the model itself will be measured by the **F1-Score**.


The model is succesful when: 

* Visualized recommendations allow case handlers to identify potential churn and get in contact with customers.

# Data understanding

## Import

Load training and test dataset (provided from kaggle - see **https://www.kaggle.com/datasets/k123vinod/insurance-churn-prediction-weekend-hackathon**)

Note: the training dataset will be used to fit the model.
The provided test dataset will be treated as new data to simulate the models performance, since the data is unlabeled. 

```{r load-training-dataset}
LINK_train <- 
  "https://raw.githubusercontent.com/NicoHenzel/Insurance-Churn-Prediction/main/Data/Train.csv"

new_training_data <- 
  read_csv(LINK_train)
```

```{r load-new-dataset}
LINK_new <- 
  "https://raw.githubusercontent.com/NicoHenzel/Insurance-Churn-Prediction/main/Data/Test.csv"

new_data <- 
  read_csv(LINK_new)
```

## First look
```{r dim-df, include=FALSE}
dim_train <- 
  dim(new_training_data)
dim_new <- 
  dim(new_data)

total_obs <- 
  dim_train[1] + dim_new[1]

train_split <- 
  round(
    100 * (dim_train[1]/total_obs)
    )
new_split <-
  round(
    100 * (dim_new[1]/total_obs)
    )
```

Look at the first rows from each dataframe.

```{r slice-training}
new_training_data %>% 
  slice_head(n=5) %>% 
  gt()
```

```{r slice-test}
new_data %>% 
  slice_head(n=5) %>% 
  gt()
```

This gives us the following informations:

1. The data seems to be clean already. Further information will be gathered in **Format Data** section.

2. The datasets are divided by a `r train_split`/`r new_split` split. The dimensions for both dataframes are the following:

* Training set:  
  observations = `r dim_train[1]`   
  variables = `r dim_train[2]`
  
* New dataset:  
  observations = `r dim_new[1]`  
  variables = `r dim_new[2]`
  
3. The `r text_spec("labels", color = "red")` column is a boolean (1 or 0) which only appears in the training data set.  
This variable can be seen as the indicator for churning, since the column doesn't appear in the given test data.  
This also means, that the provided test data can be trated as new data to simulate a churn prediction on.


## Clean data

The datasets are already clean (lowercase column names without spaces and no special characters).
This makes it easier for us, since no data cleaning needs to be performed.

## Format data
Next we will be inspecting the given data structure and check if there are any missing values in both datasets.

```{r check-format, warning=FALSE}
glimpse(new_training_data)
vis_dat(new_training_data)
glimpse(new_data)
vis_dat(new_data)
na_training <-
  sum(is.na(new_training_data))
na_new <-
  sum(is.na(new_data))
```

We can see that:

1. The datatype for every variable (feature column) in both datasets is double, although there are variables that only hold integer values (feature_7 to feature_15). In the **Data exploration** section we will inspect the difference between integer and float variables.
2. There are `r na_training` missing values in the training and `r na_new` in the test dataset.  
3. The `r text_spec("labels", color = "red")` column is formatted as numeric (dbl). It should be a factor since it is a categorial variable with two levels (1 or 0).  
Let's take a look at the `r text_spec("labels", color = "red")` variable:

```{r display-labels-variable}
new_training_data %>% 
  count(labels) %>% 
  gt
```

First the `r text_spec("labels", color = "red")` column will be renamed to `r text_spec("churn", color = "red")`. The column is then transformed into a factor type. This is needed for running the classification algorith.


```{r data-modification}
# Rename labels
df_train <-
  new_training_data %>% 
  rename(churn = labels) %>% 
# Change column type to factor
  mutate(
    churn = as.factor(churn)
  )
```

This way the churn is more accurately represented by the`r text_spec("churn", color = "red")` variable: 

```{r display-churn-variable}
df_train %>% 
  count(churn, 
        name ="churn_total") %>%
  mutate(percent = churn_total/sum(churn_total)*100,
         percent = round(percent, 2)) %>%
 gt() %>%
  tab_header(
    title = "Insurance customers churn rate ",
    subtitle = "Anonymized training sample"
  ) %>%
  cols_label(
    churn = "Churn",
    churn_total = "Amount",
    percent = "Percent"
  )
```


Note:  
No new variables will be created, since we work with anonymized data.  
The **Data exploration** section covers analysis and interpretation of the relations between the variables.

## Data overview

```{r skim-df-train}
skim(df_train) %>% 
  gt()
```

The overview shows:  

* Feature_7 - 15 are discrete and furthermore feature_10 - 12 seem to be binary (1 or 0).
* Every other feature column holds continuous values.
* The maximum and minimum values differ quite a bit, which means the scale for the variables have different scales. This issue will be solved in the **Data preparation** section through feature scaling (data normalization).
* Feature 0-6 have a positive skew shown by the histograms. This will also be addressed in the **Data preparation** section by performing transformations to approximate a normal distribution for all features.
* The shown stats (mean, sd, perecentiles) are nearly impossible to interpret at this point since we don't know what the variables itself mean.

## Data split

Before exploring the data we perform a data split. The **Data exploration** will only be done on the training set. This is crucial so we don't use any information from the training set in our model building phase. Although the data already comes with a `r train_split`/`r new_split` split the provided test data will not be used for model training since it is unlabeled and better suited to simulate new data.

```{r initial-data-split}

# Seeded split to provide the same split everytime the split is made to make sure we always use the same data for model training.
set.seed(42)
# Split the data with a 75/25 proportion in favor for the training set
data_split <- 
  initial_split(
    df_train, # original training dataset
    prop = 3/4, # 75/25 split
    strata = churn # stratified on churn variable
    )
# Create the dataframes for training and testing
train_data <- training(data_split) 
test_data <- testing(data_split)
```



## Data exploration
In order to get a better understanding of the data and underlying relations between features, we plot the data in different ways.

### Setup
We create a new dataframe to avoid altering the training dataset during the data exploration. 

```{r save-exploration-df}
explore_data <-
  train_data
```

Then we change the binary values to named values to represent the churn in order to make the plots easier to read (Using `r text_spec("yes", color = "green")` and `r text_spec("no", color = "green")` instead of 1 and 0).

```{r change-binary}
#Change column type to character to change values
explore_data <-
  explore_data %>% 
  mutate(
    churn = as.character(churn)
  ) 
# Change values
explore_data$churn[explore_data$churn == "1"] <-
  "yes"
explore_data$churn[explore_data$churn == "0"] <-
  "no"
#Change column type back to factor
explore_data <-
  explore_data %>% 
  mutate(
    churn = as.factor(churn)
  ) 

```


### Churn distribution

The ratio of customers that have churned within the training data set is depicted below.
```{r pie-chart, fig1, echo=FALSE}
explore_data %>% 
  count(churn, name ="churn_total") %>%
  mutate(percent = churn_total/sum(churn_total)*100,
         percent = round(percent, 2)) %>%
  ggplot(
  aes(x="",
    y=percent,
    fill=churn)
  ) +
  geom_bar(
    stat="identity",
    width=1
    ) +
  coord_polar("y", start = -175) +
  theme_classic() + 
  theme(
    axis.line = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank()
  ) +
  scale_fill_manual(
    values=c("#ffdb58", "#bcd4e6")
      ) + 
  labs(
    x = NULL,
    y = NULL,
    fill = NULL,
    title = "Customer churn distribution",
    subtitle = "For insurance contracts") +
  geom_text(
    aes(label = paste0(percent, "%")),
    position = position_stack(vjust=0.5)
  )
```


### Correlation to churn 

The correlation matrix helps us see different relations between the features as it may indicate a predictive relationship between variables (FUßNOTE EINFÜGEN: https://en.wikipedia.org/wiki/Correlation).
It is formed by calculating the correlation coefficient **r** between each feature. **r** ranges from -1 to 1 and indicates the strength and direction of the linear relation:  
Positive numbers indicate that greater values of one variable lead to greater values of the other variable which also holds true for lower values (similar behavior).  
Negative numbers indicate that greater values of one variable correspond to lesser values of the other (opposite behavior).  
(See: https://en.wikipedia.org/wiki/Correlation and https://en.wikipedia.org/wiki/Covariance)
```{r correlation-matrix, fig2, fig.asp = 1, fig.width = 9, echo=FALSE}


# Computing correlation coeffeicient matrix - has to be done on new_training_data since correlation is calculated only for numeric values
correlation_matrix <- 
  new_training_data %>%
  # change labels to churn for plot
  rename(churn = labels) %>% 
  # cor() uses the pearson method as default
  cor()

# creat correlation matrix with ggplot 
ggplot(
  melt(correlation_matrix),
  aes(Var1, Var2, fill=value)
  ) +
  geom_tile() +
  scale_fill_gradient2(
    low="blue",
    mid="white",
    high="gold"
    ) +
  theme_minimal() +
  coord_equal() +
  labs(
    x= "",
    y= "",
    fill= "Corr",
    title = "Correlation matrix",
    subtitle = "For all features in the training data") +
  theme(
    axis.text.x=element_text(
      size=13,
      angle=90,
      vjust=1,
      hjust=1, 
      margin=margin(-3,0,0,0)
      ),
  axis.text.y=element_text(
    size=13, 
    margin=margin(0,-3,0,0)
    ),
  panel.grid.major=element_blank()
  )

```

It is also useful to show the specific correlation of each feature to our churn variable:

```{r churn-correlation-bar-plot, fig3, echo=FALSE}
correlation_matrix %>% 
  as.data.frame() %>% 
  select(churn) %>% 
  rename(values = churn) %>% 
  tibble::rownames_to_column() %>% 
  ggplot(
    aes(x = reorder(rowname, -values),
        y = values,
        fill = values)
    )+
  geom_col() + 
  labs(
    x= "Feature",
    y= "Correlation magnitude",
    title = "Correlation to churn",
    subtitle = "In descending order"
    ) +
  # scale_fill_viridis() +
  scale_fill_gradient2(
    low="white",
    mid="blue",
    high="gold"
    ) +
  scale_y_continuous(
    breaks = seq(-0.2, 1, 0.2)
    ) +
  theme(
    axis.text.x=element_text(
      #size=10,
      angle=90,
      vjust=0.25,
      hjust=1
      )
    )

```

We can see that the correlation between `r text_spec("churn", color = "red")` and the other features is the following:

* It correlates not or very weak with most of the features  
* `r text_spec("feature_3", color = "red")` has a moderat positive correlation with `r text_spec("churn", color = "red")`.  
* `r text_spec("feature_5 & 6", color = "red")` correlate slightly positive.  
* `r text_spec("feature_11 & 13", color = "red")` correlate slightly negative.

To further inspect the relation between `r text_spec("churn", color = "red")` and other variables we will create plots for each variable.

### Float Variables

We will start with boxplots and histograms for the float variables.

```{r boxplot-multiplot-float, fig.asp=0.618, echo=FALSE}
# Credits for the code chunks goes to:
# https://www.kirenz.com/post/2021-02-17-r-classification-tidymodels/#evaluate-models

print_boxplot_float <- function(.y_var){
  
  # convert strings to variable
  y_var <- sym(.y_var) 
 
  # unquote variables using {{}}
  explore_data %>% 
    # Filter out some extreme outliers to better display the plots
    filter(
      feature_1 < 10,
      feature_4 < 8,
      feature_5 < 5,
      feature_6 < 10
      ) %>% 
  ggplot(
    aes(x = churn,
        y = {{y_var}},
        fill = churn,
        color = churn
        )) +
    geom_boxplot(alpha=0.4) +
    labs(
      x = "Churn",
      title = "Relation to churn",
      subtitle = "For every float variable"
    ) +
    theme(legend.position = "none") +
    scale_color_manual(values = c("#ffdb58", "#bcd4e6")) +
    scale_fill_grey()
} 

y_var_float <-
  explore_data %>% 
  # select feature_0 - 6 since those contain float numbers
  select(1:7) %>% 
  # obtain names
  variable.names()

map(y_var_float, print_boxplot_float)
```

```{r bar-multiplot-float, fig.asp=0.618, echo=FALSE}
# Credits for the code chunks goes to:
# https://www.kirenz.com/post/2021-02-17-r-classification-tidymodels/#evaluate-models


print_bar_float <- function(.x_var){
  
  # convert strings to variable
  x_var <- sym(.x_var) 
 
  # unquote variables using {{}}
  explore_data %>% 
  ggplot(
    aes(x = {{x_var}})) +
    geom_histogram(bins = 20, fill = "blue") +
    labs(
      y = "Frequency",
      title = "Histogram of every float variable",
      subtitle = "To check value range"
    ) 
} 

x_var_float <-
  explore_data %>% 
  # select feature_0 - 6 since those contain float numbers
  select(1:7) %>%
  # obtain names
  variable.names()

map(x_var_float, print_bar_float)
```


This confirms our previous insights from the **Data overview** and **Correlation to churn ** section:  

* Most of the variables do not affect the churn.  

* Only `r text_spec("feature_3", color = "red")` has a noticeable impact on `r text_spec("churn", color = "red")`. 

* `r text_spec("feature_5 & 6", color = "red")` can be interesting candidates for model training. This will be testes in the **Model** section

* There are alot of outliers present for almost every feature. This can be seen in the histograms - the distributions are positive skewed (Note that `r text_spec("features 1, 4, 5 & 6", color = "red")` have been filtered to better display the respective boxplots).

* The scales for float variables are very different.

### Integer Variables

Next up are the boxplots and histograms for the integer variables.

```{r boxplot-function-int, fig.asp=0.618, echo=FALSE}
# Credits for the code chunks goes to:
# https://www.kirenz.com/post/2021-02-17-r-classification-tidymodels/#evaluate-models

print_boxplot_int <- function(.y_var){
  
  # convert strings to variable
  y_var <- sym(.y_var) 
 
  # unquote variables using {{}}
  explore_data %>% 
  ggplot(
    aes(x = churn,
        y = {{y_var}},
        fill = churn,
        color = churn
        )) +
    geom_boxplot(alpha=0.4) +
    labs(
      x = "Churn",
      title = "Relation to churn",
      subtitle = "For every integer variable"
    ) +
    theme(legend.position = "none") +
    scale_color_manual(values = c("#ffdb58", "#bcd4e6")) +
    scale_fill_grey()
} 

y_var_int <-
  explore_data %>% 
  # select feature_0 - 6 since those contain float numbers
  select(8:16) %>% 
  # obtain names
  variable.names()

map(y_var_int, print_boxplot_int)
```


```{r bar-multiplot-int, fig.asp=0.618, echo=FALSE}
# Credits for the code chunks goes to:
# https://www.kirenz.com/post/2021-02-17-r-classification-tidymodels/#evaluate-models


print_bar_int <- function(.x_var){
  
  # convert strings to variable
  x_var <- sym(.x_var) 
 
  # unquote variables using {{}}
  explore_data %>% 
    # Filter out some extreme outliers to better display the plots
  ggplot(
    aes(x = {{x_var}},
 
        )) +
    geom_histogram(bins = 20, fill = "blue") +
    labs(
      y = "Frequency",
      title = "Histogram of every int variable",
      subtitle = "To check value range"
    ) 
} 

x_var_int <-
  explore_data %>% 
  # select feature_0 - 6 since those contain float numbers
  select(8:16) %>%
  # obtain names
  variable.names()

map(x_var_int, print_bar_int)
```


Again, this is a confirmation of what we have seen before:

* `r text_spec("feature 11", color = "red")` has a noticeable impact on the `r text_spec("churn", color = "red")` variable.

* `r text_spec("feature 13", color = "red")` can also be an interesting candidate for model training. This will be tested in the **Model** section.

* `r text_spec("feature 7 - 15", color = "red")` contain discrete integer values, in different levels. `r text_spec("feature 10 - 12", color = "red")` contain binary values (0 and 1) for example.

* The scales for integer variables are also different (although not to the same extent as for the float variables).


As a conclusion, we will train our model with 

* `r text_spec("feature 3", color = "red")`
* `r text_spec("feature 5", color = "red")`
* `r text_spec("feature 6", color = "red")`
* `r text_spec("feature 11", color = "red")`
* `r text_spec("feature 13", color = "red")`


# Data preparation

Before model training we will:  
* select our previous identified variables that have an effect on churn  
* create a data split with out freshly gained insights
* create a recipe to remove outliers and perform feature scaling  
* create a validation set

## Select relevant variables
Select the outcome and predictors for our model
```{r select-relevant-variables}

df_train_new <-
  df_train %>% 
  select(feature_3,
         feature_5,
         feature_6,
         feature_11,
         feature_13,
         churn)

glimpse(df_train_new)
```

Before we move on, we need to check for the minimum value of our selected variables. We can see that some values are negative. These need to be transformed into positive numbers in order to correctly apply the logarithmic transformation with `r text_spec("step_log", color = "red")` so that the distributions are more similar to a bell shape.  

```{r check-min}
# FOR LOOP SCHREIBEN
min(train_data$feature_3)
min(train_data$feature_5)
min(train_data$feature_6)
min(train_data$feature_11)
min(train_data$feature_13)

```

To solve this, we will add a (different) constant to each value of the variables `r text_spec("feature 3 - 6", color = "red")` so that the minimum for these features is 1.
This makes it possible to apply the logarithm in our recipe.

```{r}
df_train_new <-
  HIER WEITER

```



## New Data split
The data split will be done once again on the `r text_spec("df_train_new", color = "red")` in order to reflect our newest insights. Therefore we need to create another data split.

```{r data-split}
# Seeded split to provide the same split everytime the split is made to make sure we always use the same data for model training.
set.seed(42)
# Split the data with a 75/25 proportion in favor for the training set
data_split <- 
  initial_split(
    df_train_new, # updated data
    prop = 3/4, # 75/25 split
    strata = churn # stratified on churn variable
    )
# Create the dataframes for training and testing
train_data <- training(data_split) 
test_data <- testing(data_split)
```

## Validation set

For the validation set a k-fold crossvalidation with a set of 5 validation folds will be used. Also, the sample is stratified. This makes sure that every characteristic of the data is properly represented in each sample.
The validation set is used in order to check the models performance on the training dataset.

```{r evaluation-split}
# Fix random generation, also see data split
set.seed(42)
# generate the cross validation set
cv_folds <-
  vfold_cv(train_data,
           v=5, # number of folds
           strata = churn, # ensure the same proportion of the churn variable in every fold
           breaks = 4)
```


## Data preprocessing recipe

```{r}
# Create a recipe for our model
churn_rec <-
  recipe(
    # outcome ~ predictor
    churn ~ .,
    data = train_data) %>%
  # set the features that should be update in model training but not included in the results so we can test how they impact the performance of the model
  update_role(feature_5,
              feature_6,
              feature_13) %>%
  # Imputation (fehlende Variablen werden durch Median ersetzt) für alle numerischen, außer die Ergebnisvariable (y)
  # Imputationen sollten mit Vorsicht genutzt werden, da viele
   #step_impute_median(all_numeric(), -all_outcomes()) %>%
  # Imputation (fehlende Daten werden mit Mode ersetzt) für alle nominalen Prädikatoren (kategorialen Variablen)
   #step_impute_mode(all_nominal_predictors()) %>%
  # Standard Score z berechnen, um die Variablen untereinander vergleichen zu können
  # Test daten dürfen hier nicht mit normalisiert werden
  # Bei den Test daten dürfen später niemals der Mittelwert berechnet werden, stattdessen nimmt man die Mittelwerte und Standardabweichungen des Trainingsdaten, um die Test Daten zu normalisieren
   #step_normalize(all_numeric(), -all_outcomes()) %>%
  # One Hot Encoding / Bildung von Dummy Variablen - Die Zeilen der kategorialen Variablen werden in Spalten umgewandelt. Diese beinhalten binäre Werte in den Zeilen (ist die Ausprägung vorliegend, oder nicht)
   #step_dummy(all_nominal_predictors()) %>%
  # Korrelationsprüfung, damit die Abhängigkeit der Variablen untereinander geprüft werden
  step_corr(all_predictors(), threshold = 0.7, method = "spearman")

```

```{r}
# Anzeigen, was mit df_rec gemacht werden soll
summary(churn_rec)
```

```{r}
prepped_data <-
  churn_rec %>% # use the recipe object
  prep() %>% # perform the recipe on training data
  juice() # extract only the preprocessed dataframe

glimpse(prepped_data)
```


The recipe is prepared such that the churn variable serves as the outcome which the model should predict.


# Model
TO DO:
Beschreiben, wie vorgegangen wird
Unterschiedliche Modelle ausprobieren und vergleichen.
## Model specification


```{r specification}
xgb_spec <-
  # boost_tree nutzt automatisch die empirisch besten Parameter, wenn keine übergeben werden
  boost_tree() %>%
  set_engine("xgboost") %>%
  # Kann als Klassifikation, oder Regression genutzt werden, deshalb muss das spezifiziert werden
  set_mode("classification")

#show model specification
xgb_spec


```

```{r workflow}
# workflow pipeline
# In die Wf Pipeline wird das recipe aufgenommen (es können auch mehrere sein)
xgb_wflow <-
 workflow() %>%
 add_recipe(churn_rec) %>%
 add_model(xgb_spec)
# show workflow
xgb_wflow
```


## Model execution
```{r}

# Erstellt einen Fit der Daten an das Modell und prüft es mit den Folds

xgb_res <-
  xgb_wflow %>%
  fit_resamples(
    # fitte die Daten Anhand der Kreuzvalidierung mit den Folds, die wir vorab festgelegt haben
    resamples = cv_folds,
    # Speichern der Ergebnisse, um die Prediciton anzuschauen
    # Ist nicht immer notwendig
    control = control_resamples(save_pred = TRUE)
  )

```

```{r}
# Zeige die Ergebnisse des Model fits für den Algorithmus xgb an
# rmse gibt Abstand zu Fit des Modells an
# rsq gibt prozentualen Wert an zur Erklärung des Modells für die Datengrundlage
xgb_res %>% collect_metrics(summarize = FALSE)

```

## Model evaluation
TO DO
siehe https://www.kirenz.com/post/2021-02-17-r-classification-tidymodels/#boosted-tree-xgboost


## Model Tuning

Siehe
https://htmlpreview.github.io/?https://github.com/kirenz/tidymodels-in-r/blob/main/05-tidymodels-xgboost-tuning.html

# Last evaluation (on test set)

<!-- ```{r} -->
<!-- # Fitte das Modell an die Test Daten, zur Prüfung der Aussagekraft -->
<!-- # Zeigt die Ergebnisse des Models an, indem Daten geprüft werden, die das Model noch nicht kennt -->
<!-- last_fit_xgb <- last_fit(xgb_wflow, split = data_split) -->

<!-- # Show RMSE and RSQ -->
<!-- last_fit_xgb %>% -->
<!--   collect_metrics() -->

<!-- ``` -->

# Additional interpretation
We have seen that our model performs ....
Since the data provided was anonymized, I used some additional research in order to give an overview of potential features that impact the churn rate.
To DO:
Research und relevante Features beschreiben
## Relevant features

<!-- Siehe Google Scholar Veröffentlichungen für relevante Features, da Daten anonym -->
